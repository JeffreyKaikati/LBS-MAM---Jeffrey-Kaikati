---
categories:
- ""
- ""
date: "2017-10-31T22:42:51-05:00"
description: Nullam et orci eu lorem consequat tincidunt vivamus et sagittis magna
  sed nunc rhoncus condimentum sem. In efficitur ligula tate urna. Maecenas massa
  sed magna lacinia magna pellentesque lorem ipsum dolor. Nullam et orci eu lorem
  consequat tincidunt. Vivamus et sagittis tempus.
draft: false
image: pic07.jpg
keywords: ""
slug: homework2
title: Homework 2
---

<script src="LBS2-homework2-Group7 copia_files/header-attrs/header-attrs.js"></script>


<div id="climate-change-and-temperature-anomalies" class="section level1">
<h1>Climate change and temperature anomalies</h1>
<p>If we wanted to study climate change, we can find data on the <em>Combined Land-Surface Air and Sea-Surface Water Temperature Anomalies</em> in the Northern Hemisphere at <a href="https://data.giss.nasa.gov/gistemp">NASA’s Goddard Institute for Space Studies</a>. The <a href="https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.txt">tabular data of temperature anomalies can be found here</a></p>
<p>To define temperature anomalies you need to have a reference, or base, period which NASA clearly states that it is the period between 1951-1980.</p>
<p>Run the code below to load the file:</p>
<pre class="r"><code>weather &lt;- 
  read_csv(&quot;https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv&quot;, 
           skip = 1, 
           na = &quot;***&quot;)</code></pre>
<p>Notice that, when using this function, we added two options: <code>skip</code> and <code>na</code>.</p>
<ol style="list-style-type: decimal">
<li>The <code>skip=1</code> option is there as the real data table only starts in Row 2, so we need to skip one row.</li>
<li><code>na = "***"</code> option informs R how missing observations in the spreadsheet are coded. When looking at the spreadsheet, you can see that missing data is coded as "***". It is best to specify this here, as otherwise some of the data is not recognized as numeric data.</li>
</ol>
<p>For each month and year, the dataframe “weather” shows the deviation of temperature from the normal (expected). Further the dataframe is in wide format.</p>
<p>You have two objectives in this section:</p>
<ol style="list-style-type: decimal">
<li>Select the year and the twelve month variables from the <code>weather</code> dataset. We do not need the others (J-D, D-N, DJF, etc.) for this assignment. Hint: use <code>select()</code> function.</li>
</ol>
<pre class="r"><code>weather %&gt;% 
  select(1:13) </code></pre>
<pre><code>## # A tibble: 142 × 13
##     Year   Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1  1880 -0.34 -0.5  -0.22 -0.29 -0.05 -0.15 -0.17 -0.25 -0.22 -0.31 -0.42 -0.39
##  2  1881 -0.3  -0.21 -0.03  0.01  0.04 -0.32  0.09 -0.03 -0.25 -0.42 -0.36 -0.22
##  3  1882  0.27  0.22  0.02 -0.31 -0.24 -0.29 -0.27 -0.14 -0.24 -0.52 -0.32 -0.68
##  4  1883 -0.57 -0.65 -0.15 -0.3  -0.25 -0.11 -0.05 -0.22 -0.33 -0.16 -0.44 -0.14
##  5  1884 -0.16 -0.1  -0.64 -0.59 -0.35 -0.41 -0.44 -0.5  -0.44 -0.44 -0.57 -0.46
##  6  1885 -1    -0.44 -0.23 -0.48 -0.58 -0.44 -0.34 -0.41 -0.4  -0.37 -0.38 -0.11
##  7  1886 -0.74 -0.83 -0.72 -0.37 -0.33 -0.37 -0.15 -0.43 -0.33 -0.31 -0.39 -0.22
##  8  1887 -1.08 -0.7  -0.44 -0.38 -0.25 -0.2  -0.24 -0.54 -0.21 -0.49 -0.27 -0.43
##  9  1888 -0.49 -0.61 -0.64 -0.22 -0.15 -0.03 -0.01 -0.21 -0.2  -0.04 -0.01 -0.24
## 10  1889 -0.28  0.29 -0.02  0.16 -0.04 -0.07 -0.09 -0.21 -0.3  -0.42 -0.62 -0.54
## # … with 132 more rows</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Convert the dataframe from wide to ‘long’ format. Hint: use <code>gather()</code> or <code>pivot_longer()</code> function. Name the new dataframe as <code>tidyweather</code>, name the variable containing the name of the month as <code>month</code>, and the temperature deviation values as <code>delta</code>.</li>
</ol>
<pre class="r"><code>tidyweather &lt;- weather %&gt;% 
             select(1:13) %&gt;% 
            pivot_longer(cols=2:13,
             names_to=&quot;Month&quot;,
             values_to = &quot;delta&quot;)
tidyweather</code></pre>
<pre><code>## # A tibble: 1,704 × 3
##     Year Month delta
##    &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;
##  1  1880 Jan   -0.34
##  2  1880 Feb   -0.5 
##  3  1880 Mar   -0.22
##  4  1880 Apr   -0.29
##  5  1880 May   -0.05
##  6  1880 Jun   -0.15
##  7  1880 Jul   -0.17
##  8  1880 Aug   -0.25
##  9  1880 Sep   -0.22
## 10  1880 Oct   -0.31
## # … with 1,694 more rows</code></pre>
<p>Inspect your dataframe. It should have three variables now, one each for</p>
<ol style="list-style-type: decimal">
<li>year,</li>
<li>month, and</li>
<li>delta, or temperature deviation.</li>
</ol>
<div id="plotting-information" class="section level2">
<h2>Plotting Information</h2>
<p>Let us plot the data using a time-series scatter plot, and add a trendline. To do that, we first need to create a new variable called <code>date</code> in order to ensure that the <code>delta</code> values are plot chronologically.</p>
<pre class="r"><code>tidyweather &lt;- tidyweather %&gt;%
  mutate(date = ymd(paste(as.character(Year), Month, &quot;1&quot;)),
         month = month(date, label=TRUE),
         year = year(date))

ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point()+
  geom_smooth(color=&quot;red&quot;) +
  theme_bw() +
  labs (
    title = &quot;Weather Anomalies&quot;)</code></pre>
<p><img src="/blogs/LBS2-homework2-Group7%20copia_files/figure-html/scatter_plot-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>Is the effect of increasing temperature more pronounced in some months? Use <code>facet_wrap()</code> to produce a seperate scatter plot for each month, again with a smoothing line. Your chart should human-readable labels; that is, each month should be labeled “Jan”, “Feb”, “Mar” (full or abbreviated month names are fine), not <code>1</code>, <code>2</code>, <code>3</code>.</p>
<blockquote>
<p>The effect of increasing temperature seem to be more pronounced especially during the winter period, for the months of December, February, January, and March.</p>
</blockquote>
<pre class="r"><code>tidyweather &lt;- tidyweather %&gt;%
  mutate(date = ymd(paste(as.character(Year), Month, &quot;1&quot;)),
         month = month(date, label=TRUE),
         year = year(date))

ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point()+
  geom_smooth(color=&quot;red&quot;) +
  facet_wrap(&quot;Month&quot;)+
  theme_bw() +
  labs (
    title = &quot;Weather Anomalies per Month&quot;
  )</code></pre>
<p><img src="/blogs/LBS2-homework2-Group7%20copia_files/figure-html/facet_wrap-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>It is sometimes useful to group data into different time periods to study historical data. For example, we often refer to decades such as 1970s, 1980s, 1990s etc. to refer to a period of time. NASA calculaltes a temperature anomaly, as difference form the base period of 1951-1980. The code below creates a new data frame called <code>comparison</code> that groups data in five time periods: 1881-1920, 1921-1950, 1951-1980, 1981-2010 and 2011-present.</p>
<p>We remove data before 1800 and before using <code>filter</code>. Then, we use the <code>mutate</code> function to create a new variable <code>interval</code> which contains information on which period each observation belongs to. We can assign the different periods using <code>case_when()</code>.</p>
<pre class="r"><code>comparison &lt;- tidyweather %&gt;% 
  filter(Year&gt;= 1881) %&gt;%     #remove years prior to 1881
  mutate(interval = case_when(
    Year %in% c(1881:1920) ~ &quot;1881-1920&quot;,
    Year %in% c(1921:1950) ~ &quot;1921-1950&quot;,
    Year %in% c(1951:1980) ~ &quot;1951-1980&quot;,
    Year %in% c(1981:2010) ~ &quot;1981-2010&quot;,
    TRUE ~ &quot;2011-present&quot;
  ))</code></pre>
<p>Inspect the <code>comparison</code> dataframe by clicking on it in the <code>Environment</code> pane.</p>
<p>Now that we have the <code>interval</code> variable, we can create a density plot to study the distribution of monthly deviations (<code>delta</code>), grouped by the different time periods we are interested in. Set <code>fill</code> to <code>interval</code> to group and colour the data by different time periods.</p>
<pre class="r"><code>ggplot(comparison, aes(x=delta, fill=interval))+
  geom_density(alpha=0.2) +   #density plot with tranparency set to 20%
  theme_bw() +                #theme
  labs (
    title = &quot;Density Plot for Monthly Temperature Anomalies&quot;,
    y     = &quot;Density&quot;         #changing y-axis label to sentence case
  )</code></pre>
<p><img src="/blogs/LBS2-homework2-Group7%20copia_files/figure-html/density_plot-1.png" width="648" style="display: block; margin: auto;" /></p>
<p>So far, we have been working with monthly anomalies. However, we might be interested in average annual anomalies. We can do this by using <code>group_by()</code> and <code>summarise()</code>, followed by a scatter plot to display the result.</p>
<pre class="r"><code>#creating yearly averages
average_annual_anomaly &lt;- tidyweather %&gt;% 
  group_by(Year) %&gt;%   
   summarise(annual_average_delta = mean(delta),  na.rm = TRUE) 

#plotting the data:
ggplot(average_annual_anomaly, aes(x=Year, y= annual_average_delta))+
  geom_point()+
  geom_smooth() +
  #change to theme_bw() to have white background + black frame around plot
  theme_bw() +
  labs (
    title = &quot;Average Yearly Anomaly&quot;,
    y     = &quot;Average Annual Delta&quot;
  )                         </code></pre>
<p><img src="/blogs/LBS2-homework2-Group7%20copia_files/figure-html/averaging-1.png" width="648" style="display: block; margin: auto;" /></p>
</div>
<div id="confidence-interval-for-delta" class="section level2">
<h2>Confidence Interval for <code>delta</code></h2>
<p><a href="https://earthobservatory.nasa.gov/world-of-change/decadaltemp.php">NASA points out on their website</a> that a one-degree global change is significant because it takes a vast amount of heat to warm all the oceans, atmosphere, and land by that much. In the past, a one- to two-degree drop was all it took to plunge the Earth into the Little Ice Age.</p>
<p>Your task is to construct a confidence interval for the average annual delta since 2011, both using a formula and using a bootstrap simulation with the <code>infer</code> package. Recall that the dataframe <code>comparison</code> has already grouped temperature anomalies according to time intervals; we are only interested in what is happening between 2011-present.</p>
<pre class="r"><code>formula_ci &lt;- comparison %&gt;% 
  # choose the interval 2011-present
  filter(Year &gt;=2011)%&gt;% 
  
  summarise(mean = mean(delta,  na.rm = TRUE), SD = sd(delta, na.rm = TRUE), count=n(),
  t_critical = qt(0.975, count-1), 
  SE = SD/sqrt(count),
  margin_error = t_critical * SE,
  lower = mean - margin_error,
  upper = mean + margin_error)

formula_ci</code></pre>
<pre><code>## # A tibble: 1 × 8
##    mean    SD count t_critical     SE margin_error lower upper
##   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  1.06 0.274   132       1.98 0.0239       0.0473  1.01  1.11</code></pre>
<pre class="r"><code>set.seed(12345)

boot_weather &lt;- comparison %&gt;%
  specify(response =delta ) %&gt;%
  generate (reps = 1000, type = &quot;bootstrap&quot;) %&gt;% 
  calculate(stat = c(&quot;mean&quot;))

boot_weather_CI &lt;- boot_weather %&gt;%
  get_confidence_interval(level=0.95, type = &quot;percentile&quot;)
boot_weather_CI</code></pre>
<pre><code>## # A tibble: 1 × 2
##   lower_ci upper_ci
##      &lt;dbl&gt;    &lt;dbl&gt;
## 1   0.0568    0.102</code></pre>
<pre class="r"><code>formula_ci %&gt;%
  select(lower,upper)</code></pre>
<pre><code>## # A tibble: 1 × 2
##   lower upper
##   &lt;dbl&gt; &lt;dbl&gt;
## 1  1.01  1.11</code></pre>
<pre class="r"><code>boot_weather_CI</code></pre>
<pre><code>## # A tibble: 1 × 2
##   lower_ci upper_ci
##      &lt;dbl&gt;    &lt;dbl&gt;
## 1   0.0568    0.102</code></pre>
<p>What does the data show?</p>
<blockquote>
<p>The data gives us very two different confidence intervals for the mean change in temperatures over the years.
The two confidence intervalls do not overlap, which suggests that one of the method is getting flawed results.
Even if the data is not normally distributed, bootstrapping provides us with a normalised distribution of the mean, meaning that it will be able to correct the skewness of the initial data. However, if the initial sample was flawed, it will mean that the data is distributed around the wrong mean, with a wrong standard deviation.</p>
</blockquote>
</div>
</div>
<div id="general-social-survey-gss" class="section level1">
<h1>General Social Survey (GSS)</h1>
<p>The <a href="http://www.gss.norc.org/">General Social Survey (GSS)</a> gathers data on American society in order to monitor and explain trends in attitudes, behaviours, and attributes. Many trends have been tracked for decades, so one can see the evolution of attitudes, etc in American Society.</p>
<p>In this assignment we analyze data from the <strong>2016 GSS sample data</strong>, using it to estimate values of <em>population parameters</em> of interest about US adults. The GSS sample data file has 2867 observations of 935 variables, but we are only interested in very few of these variables and you are using a smaller file.</p>
<pre class="r"><code>gss &lt;- read_csv(here::here(&quot;data&quot;, &quot;smallgss2016.csv&quot;), 
                na = c(&quot;&quot;, &quot;Don&#39;t know&quot;,
                       &quot;No answer&quot;, &quot;Not applicable&quot;))</code></pre>
<p>You will also notice that many responses should not be taken into consideration, like “No Answer”, “Don’t Know”, “Not applicable”, “Refused to Answer”.</p>
<p>We will be creating 95% confidence intervals for population parameters. The variables we have are the following:</p>
<ul>
<li>hours and minutes spent on email weekly. The responses to these questions are recorded in the <code>emailhr</code> and <code>emailmin</code> variables. For example, if the response is 2.50 hours, this would be recorded as emailhr = 2 and emailmin = 30.</li>
<li><code>snapchat</code>, <code>instagrm</code>, <code>twitter</code>: whether respondents used these social media in 2016</li>
<li><code>sex</code>: Female - Male</li>
<li><code>degree</code>: highest education level attained</li>
</ul>
<div id="instagram-and-snapchat-by-sex" class="section level2">
<h2>Instagram and Snapchat, by sex</h2>
<p>Can we estimate the <em>population</em> proportion of Snapchat or Instagram users in 2016?</p>
<ol style="list-style-type: decimal">
<li>Create a new variable, <code>snap_insta</code> that is <em>Yes</em> if the respondent reported using any of Snapchat (<code>snapchat</code>) or Instagram (<code>instagrm</code>), and <em>No</em> if not. If the recorded value was NA for both of these questions, the value in your new variable should also be NA.</li>
</ol>
<pre class="r"><code>glimpse(gss)</code></pre>
<pre><code>## Rows: 2,867
## Columns: 7
## $ emailmin &lt;chr&gt; &quot;0&quot;, &quot;30&quot;, &quot;NA&quot;, &quot;10&quot;, &quot;NA&quot;, &quot;0&quot;, &quot;0&quot;, &quot;NA&quot;, &quot;0&quot;, &quot;NA&quot;, &quot;0&quot;, …
## $ emailhr  &lt;chr&gt; &quot;12&quot;, &quot;0&quot;, &quot;NA&quot;, &quot;0&quot;, &quot;NA&quot;, &quot;2&quot;, &quot;40&quot;, &quot;NA&quot;, &quot;0&quot;, &quot;NA&quot;, &quot;2&quot;, …
## $ snapchat &lt;chr&gt; &quot;NA&quot;, &quot;No&quot;, &quot;No&quot;, &quot;NA&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;NA&quot;, &quot;Yes&quot;, &quot;NA&quot;, &quot;No&quot;,…
## $ instagrm &lt;chr&gt; &quot;NA&quot;, &quot;No&quot;, &quot;No&quot;, &quot;NA&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;NA&quot;, &quot;Yes&quot;, &quot;NA&quot;, &quot;No&quot;…
## $ twitter  &lt;chr&gt; &quot;NA&quot;, &quot;No&quot;, &quot;No&quot;, &quot;NA&quot;, &quot;No&quot;, &quot;No&quot;, &quot;NA&quot;, &quot;No&quot;, &quot;NA&quot;, &quot;No&quot;, &quot;…
## $ sex      &lt;chr&gt; &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;,…
## $ degree   &lt;chr&gt; &quot;Bachelor&quot;, &quot;High school&quot;, &quot;Bachelor&quot;, &quot;High school&quot;, &quot;Gradua…</code></pre>
<pre class="r"><code>gss_combined &lt;- gss %&gt;%
  mutate(snapchat = recode(snapchat, &quot;No&quot;=0, &quot;Yes&quot;=1)) %&gt;%
  mutate(instagrm = recode(instagrm, &quot;No&quot;=0, &quot;Yes&quot;=1))%&gt;%
  mutate(snap_insta_temporary = snapchat+instagrm)%&gt;%
  mutate(snap_insta = ceiling(snap_insta_temporary/2))</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Calculate the proportion of Yes’s for <code>snap_insta</code> among those who answered the question, i.e. excluding NAs.</li>
</ol>
<pre class="r"><code>gss_combined %&gt;%
  summarise(yesses= count(snap_insta==1), noses=count(snap_insta==0), proportion=yesses/noses)</code></pre>
<pre><code>## # A tibble: 1 × 3
##   yesses noses proportion
##    &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;
## 1    514   858      0.599</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Using the CI formula for proportions, please construct 95% CIs for men and women who used either Snapchat or Instagram</li>
</ol>
<pre class="r"><code>formula_ci_FM &lt;- gss_combined %&gt;%
  group_by(sex) %&gt;%
  filter(na.rm=TRUE) %&gt;%
  summarise(mean = mean(snap_insta,  na.rm = TRUE), SD = sd(snap_insta, na.rm = TRUE), count=n(),
  t_critical = qt(0.975, count-1), 
  SE = SD/sqrt(count),
  margin_error = t_critical * SE,
  lower = mean - margin_error,
  upper = mean + margin_error)

formula_ci_FM</code></pre>
<pre><code>## # A tibble: 2 × 9
##   sex     mean    SD count t_critical     SE margin_error lower upper
##   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 Female 0.419 0.494  1591       1.96 0.0124       0.0243 0.394 0.443
## 2 Male   0.318 0.466  1276       1.96 0.0131       0.0256 0.293 0.344</code></pre>
</div>
<div id="twitter-by-education-level" class="section level2">
<h2>Twitter, by education level</h2>
<p>Can we estimate the <em>population</em> proportion of Twitter users by education level in 2016?.</p>
<p>There are 5 education levels in variable <code>degree</code> which, in ascending order of years of education, are Lt high school, High School, Junior college, Bachelor, Graduate.</p>
<ol style="list-style-type: decimal">
<li>Turn <code>degree</code> from a character variable into a factor variable. Make sure the order is the correct one and that levels are not sorted alphabetically which is what R by default does.</li>
</ol>
<pre class="r"><code>gss_combined &lt;- gss_combined %&gt;%
  mutate(degree = factor(degree, levels = c(&quot;Lt high school&quot;, &quot;High School&quot;, &quot;Junior college&quot;, &quot;Bachelor&quot;, &quot;Graduate&quot;)))%&gt;%
  arrange(degree)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Create a new variable, <code>bachelor_graduate</code> that is <em>Yes</em> if the respondent has either a <code>Bachelor</code> or <code>Graduate</code> degree. As before, if the recorded value for either was NA, the value in your new variable should also be NA.</li>
</ol>
<pre class="r"><code>gss_combined &lt;- gss_combined %&gt;% 
  mutate(bachelor_graduate = if_else(degree %in% c(&quot;Bachelor&quot;, &quot;Graduate&quot;), &quot;Yes&quot;, if_else(degree %in% c(&quot;Lt high school&quot;, &quot;High School&quot;, &quot;Junior college&quot;), &quot;No&quot;, &quot;NA&quot;)))

gss_combined</code></pre>
<pre><code>## # A tibble: 2,867 × 10
##    emailmin emailhr snapchat instagrm twitter sex    degree         snap_insta_temp…
##    &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;fct&gt;                     &lt;dbl&gt;
##  1 30       0              0        0 No      Male   Lt high school                0
##  2 0        1             NA       NA NA      Female Lt high school               NA
##  3 NA       NA             0        1 No      Female Lt high school                1
##  4 0        1              1        1 No      Female Lt high school                2
##  5 NA       NA            NA       NA NA      Female Lt high school               NA
##  6 45       0             NA       NA NA      Female Lt high school               NA
##  7 NA       NA             0        1 No      Male   Lt high school                1
##  8 20       0             NA       NA NA      Female Lt high school               NA
##  9 NA       NA            NA       NA NA      Female Lt high school               NA
## 10 NA       NA            NA       NA NA      Female Lt high school               NA
## # … with 2,857 more rows, and 2 more variables: snap_insta &lt;dbl&gt;,
## #   bachelor_graduate &lt;chr&gt;</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Calculate the proportion of <code>bachelor_graduate</code> who do (Yes) and who don’t (No) use twitter.</li>
</ol>
<pre class="r"><code>gss_twitter &lt;- gss_combined %&gt;%
  filter(bachelor_graduate== &quot;Yes&quot;) %&gt;%
  summarise(twitter_yes= count(twitter==&quot;Yes&quot;), twitter_no=count(twitter==&quot;No&quot;), twitter_total=n(), proportion_twitter=twitter_yes/twitter_total, proportion_twitter_no=twitter_no/twitter_total)

proportion_twitter &lt;- as.numeric(gss_twitter[&quot;proportion_twitter&quot;])
proportion_twitter_no &lt;- as.numeric(gss_twitter[&quot;proportion_twitter_no&quot;])
gss_twitter</code></pre>
<pre><code>## # A tibble: 1 × 5
##   twitter_yes twitter_no twitter_total proportion_twitter proportion_twitter_no
##         &lt;int&gt;      &lt;int&gt;         &lt;int&gt;              &lt;dbl&gt;                 &lt;dbl&gt;
## 1         114        375           854              0.133                 0.439</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Using the CI formula for proportions, please construct two 95% CIs for <code>bachelor_graduate</code> vs whether they use (Yes) and don’t (No) use twitter.</li>
</ol>
<pre class="r"><code>formula_ci_twitter &lt;- gss_combined %&gt;% 
  
  filter(bachelor_graduate == &quot;Yes&quot;)%&gt;%
  summarise(count = n(),
            SE = sqrt(proportion_twitter*(1-proportion_twitter)/count),
            t_critical = qt(0.975, count-1),
            margin_error= t_critical*SE,
            lower = (proportion_twitter - margin_error),
            upper =(proportion_twitter + margin_error))
formula_ci_twitter</code></pre>
<pre><code>## # A tibble: 1 × 6
##   count     SE t_critical margin_error lower upper
##   &lt;int&gt;  &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1   854 0.0116       1.96       0.0228 0.111 0.156</code></pre>
<pre class="r"><code>formula_ci_twitter_no &lt;- gss_combined %&gt;% 
  
  filter(bachelor_graduate == &quot;Yes&quot;)%&gt;%
  summarise(count = n(),
            SE = sqrt(proportion_twitter_no*(1-proportion_twitter_no)/count),
            t_critical = qt(0.975, count-1),
            margin_error= t_critical*SE,
            lower = (proportion_twitter_no - margin_error),
            upper =(proportion_twitter_no + margin_error))

formula_ci_twitter_no</code></pre>
<pre><code>## # A tibble: 1 × 6
##   count     SE t_critical margin_error lower upper
##   &lt;int&gt;  &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1   854 0.0170       1.96       0.0333 0.406 0.472</code></pre>
<p>Do these two Confidence Intervals overlap?</p>
<blockquote>
<p>The two CIs do not overlap, so we know that there is a significant difference between the two groups, we do not need to run a t-test.</p>
</blockquote>
</div>
<div id="email-usage" class="section level2">
<h2>Email usage</h2>
<p>Can we estimate the <em>population</em> parameter on time spent on email weekly?</p>
<ol style="list-style-type: decimal">
<li>Create a new variable called <code>email</code> that combines <code>emailhr</code> and <code>emailmin</code> to reports the number of minutes the respondents spend on email weekly.</li>
</ol>
<pre class="r"><code>gss_combined &lt;- gss_combined %&gt;%
          mutate(emailmin = as.numeric(emailmin))%&gt;%
          mutate(emailhr = as.numeric(emailhr))%&gt;%
          mutate(email = emailmin + emailhr*60)

gss_combined</code></pre>
<pre><code>## # A tibble: 2,867 × 11
##    emailmin emailhr snapchat instagrm twitter sex    degree         snap_insta_temp…
##       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;fct&gt;                     &lt;dbl&gt;
##  1       30       0        0        0 No      Male   Lt high school                0
##  2        0       1       NA       NA NA      Female Lt high school               NA
##  3       NA      NA        0        1 No      Female Lt high school                1
##  4        0       1        1        1 No      Female Lt high school                2
##  5       NA      NA       NA       NA NA      Female Lt high school               NA
##  6       45       0       NA       NA NA      Female Lt high school               NA
##  7       NA      NA        0        1 No      Male   Lt high school                1
##  8       20       0       NA       NA NA      Female Lt high school               NA
##  9       NA      NA       NA       NA NA      Female Lt high school               NA
## 10       NA      NA       NA       NA NA      Female Lt high school               NA
## # … with 2,857 more rows, and 3 more variables: snap_insta &lt;dbl&gt;,
## #   bachelor_graduate &lt;chr&gt;, email &lt;dbl&gt;</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Visualise the distribution of this new variable. Find the mean and the median number of minutes respondents spend on email weekly. Is the mean or the median a better measure of the typical among of time Americans spend on email weekly? Why?</li>
</ol>
<pre class="r"><code>gss_combined %&gt;%
  ggplot(aes(x = email)) +
  geom_density(fill = &quot;cyan&quot;)</code></pre>
<p><img src="/blogs/LBS2-homework2-Group7%20copia_files/figure-html/unnamed-chunk-14-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>gss_combined %&gt;%
  summarise(mean = mean(email, na.rm = TRUE), median = median(email,na.rm = TRUE))</code></pre>
<pre><code>## # A tibble: 1 × 2
##    mean median
##   &lt;dbl&gt;  &lt;dbl&gt;
## 1  417.    120</code></pre>
<pre class="r"><code>gss_combined %&gt;%
  summarise(max=max(email,  na.rm = TRUE), min=min(email,  na.rm = TRUE))</code></pre>
<pre><code>## # A tibble: 1 × 2
##     max   min
##   &lt;dbl&gt; &lt;dbl&gt;
## 1  6000     0</code></pre>
<blockquote>
<p>The mean is heavily influenced by outliars(for instance, by skimming the data we can see how Someone spent 100 hours/week on social media) so we think that the median would be a better measure for the typical amount of time spent on email weekly.</p>
</blockquote>
<ol start="3" style="list-style-type: decimal">
<li>Using the <code>infer</code> package, calculate a 95% bootstrap confidence interval for the mean amount of time Americans spend on email weekly. Interpret this interval in context of the data, reporting its endpoints in “humanized” units (e.g. instead of 108 minutes, report 1 hr and 8 minutes). If you get a result that seems a bit odd, discuss why you think this might be the case.</li>
</ol>
<pre class="r"><code>set.seed(45678)
boot_email &lt;- gss_combined %&gt;%
  specify(response =email ) %&gt;%
  generate (reps = 1000, type = &quot;bootstrap&quot;) %&gt;% 
  calculate(stat = c(&quot;mean&quot;))

boot_email_CI &lt;- boot_email %&gt;%
  get_confidence_interval(level=0.95, type = &quot;percentile&quot;)
boot_email_CI</code></pre>
<pre><code>## # A tibble: 1 × 2
##   lower_ci upper_ci
##      &lt;dbl&gt;    &lt;dbl&gt;
## 1     385.     449.</code></pre>
<pre class="r"><code>boot_email_CI%&gt;%
  mutate(lower_hour = lower_ci%/%60)%&gt;%
  mutate(lower_minute = lower_ci %%60)%&gt;%
  mutate(upper_hour = upper_ci%/%60)%&gt;%
  mutate(upper_minute = upper_ci %%60)</code></pre>
<pre><code>## # A tibble: 1 × 6
##   lower_ci upper_ci lower_hour lower_minute upper_hour upper_minute
##      &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;
## 1     385.     449.          6         25.2          7         29.1</code></pre>
<pre class="r"><code>boot_email_CI %&gt;% 
  mutate(lower_hours = floor(lower_ci/60),
         lower_minutes = floor(lower_ci-lower_hours*60),
         lower_time = paste(as.character(lower_hours),&quot;h&quot;,as.character(lower_minutes),&quot;min&quot;),
         upper_hours= floor(upper_ci/60),
         upper_minutes = ceiling(upper_ci-upper_hours*60),
         upper_time =paste(as.character(upper_hours),&quot;h&quot;, as.character(upper_minutes),(&quot;min&quot;))) %&gt;%
         select(lower_time, upper_time)</code></pre>
<pre><code>## # A tibble: 1 × 2
##   lower_time upper_time
##   &lt;chr&gt;      &lt;chr&gt;     
## 1 6 h 25 min 7 h 30 min</code></pre>
<p>Would you expect a 99% confidence interval to be wider or narrower than the interval you calculated above? Explain your reasoning.</p>
<blockquote>
<p>We would expect the confidence interval to be wider than the 95% CI. The reason for this lies behind the fact that it comprehends more values than 95%. A 99% CI would provide us with a range of values that we can be 99% (&gt;95%) certain contains the true mean of the population, so it will clearly be wider.</p>
</blockquote>
</div>
</div>
<div id="bidens-approval-margins" class="section level1">
<h1>Biden’s Approval Margins</h1>
<p>As we saw in class, fivethirtyeight.com has detailed data on <a href="https://projects.fivethirtyeight.com/biden-approval-ratings">all polls that track the president’s approval</a></p>
<pre class="r"><code># Import approval polls data directly off fivethirtyeight website
approval_polllist &lt;- read_csv(&#39;https://projects.fivethirtyeight.com/biden-approval-data/approval_polllist.csv&#39;) 

glimpse(approval_polllist)</code></pre>
<pre><code>## Rows: 1,598
## Columns: 22
## $ president           &lt;chr&gt; &quot;Joseph R. Biden Jr.&quot;, &quot;Joseph R. Biden Jr.&quot;, &quot;Jos…
## $ subgroup            &lt;chr&gt; &quot;All polls&quot;, &quot;All polls&quot;, &quot;All polls&quot;, &quot;All polls&quot;…
## $ modeldate           &lt;chr&gt; &quot;9/14/2021&quot;, &quot;9/14/2021&quot;, &quot;9/14/2021&quot;, &quot;9/14/2021&quot;…
## $ startdate           &lt;chr&gt; &quot;1/21/2021&quot;, &quot;1/27/2021&quot;, &quot;1/27/2021&quot;, &quot;1/28/2021&quot;…
## $ enddate             &lt;chr&gt; &quot;2/2/2021&quot;, &quot;1/29/2021&quot;, &quot;1/31/2021&quot;, &quot;1/29/2021&quot;,…
## $ pollster            &lt;chr&gt; &quot;Gallup&quot;, &quot;IBD/TIPP&quot;, &quot;Rasmussen Reports/Pulse Opi…
## $ grade               &lt;chr&gt; &quot;B+&quot;, &quot;A+&quot;, &quot;B&quot;, &quot;B+&quot;, &quot;B&quot;, &quot;B/C&quot;, &quot;B-&quot;, NA, &quot;B/C&quot;…
## $ samplesize          &lt;dbl&gt; 906, 1261, 1500, 945, 6467, 841, 1200, 1055, 1005,…
## $ population          &lt;chr&gt; &quot;a&quot;, &quot;a&quot;, &quot;lv&quot;, &quot;rv&quot;, &quot;a&quot;, &quot;lv&quot;, &quot;rv&quot;, &quot;a&quot;, &quot;rv&quot;, …
## $ weight              &lt;dbl&gt; 1.3147, 2.3555, 0.1925, 1.0223, 0.1213, 1.1151, 0.…
## $ influence           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ approve             &lt;dbl&gt; 57.0, 53.0, 49.0, 61.0, 55.3, 56.1, 58.0, 61.0, 53…
## $ disapprove          &lt;dbl&gt; 37.0, 29.0, 46.0, 39.0, 33.0, 36.4, 35.0, 38.0, 39…
## $ adjusted_approve    &lt;dbl&gt; 56.4, 53.4, 51.4, 56.7, 53.9, 54.6, 57.1, 57.4, 52…
## $ adjusted_disapprove &lt;dbl&gt; 36.3, 33.1, 40.1, 39.9, 36.2, 37.0, 36.1, 38.4, 38…
## $ multiversions       &lt;chr&gt; NA, NA, NA, NA, &quot;*&quot;, NA, NA, NA, NA, NA, NA, NA, N…
## $ tracking            &lt;lgl&gt; NA, NA, TRUE, NA, TRUE, NA, NA, NA, NA, NA, TRUE, …
## $ url                 &lt;chr&gt; &quot;https://news.gallup.com/poll/329348/biden-begins-…
## $ poll_id             &lt;dbl&gt; 74344, 74321, 74293, 74322, 74323, 74326, 74355, 7…
## $ question_id         &lt;dbl&gt; 139651, 139560, 139520, 139562, 139563, 139569, 13…
## $ createddate         &lt;chr&gt; &quot;2/4/2021&quot;, &quot;2/1/2021&quot;, &quot;2/1/2021&quot;, &quot;2/1/2021&quot;, &quot;2…
## $ timestamp           &lt;chr&gt; &quot;09:40:09 14 Sep 2021&quot;, &quot;09:40:09 14 Sep 2021&quot;, &quot;0…</code></pre>
<pre class="r"><code>approval_weeks &lt;- approval_polllist %&gt;%
          mutate(week=week(mdy(enddate))-3) %&gt;%
          group_by(week) %&gt;%
          select(!enddate) %&gt;%
          select(!startdate) %&gt;%
          mutate(net_approval= approve - disapprove)%&gt;%
          select(!approve)%&gt;%
          select(!disapprove)%&gt;%
          select(!multiversions) %&gt;%
          select(!tracking)%&gt;%
          select(!adjusted_approve)%&gt;%
          select(!adjusted_disapprove)%&gt;%
          select(!timestamp)
approval_weeks</code></pre>
<pre><code>## # A tibble: 1,598 × 15
## # Groups:   week [33]
##    president  subgroup  modeldate pollster    grade samplesize population weight
##    &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;
##  1 Joseph R.… All polls 9/14/2021 Gallup      B+          906  a           1.31 
##  2 Joseph R.… All polls 9/14/2021 IBD/TIPP    A+         1261  a           2.36 
##  3 Joseph R.… All polls 9/14/2021 Rasmussen … B          1500  lv          0.192
##  4 Joseph R.… All polls 9/14/2021 HarrisX     B+          945  rv          1.02 
##  5 Joseph R.… All polls 9/14/2021 Morning Co… B          6467. a           0.121
##  6 Joseph R.… All polls 9/14/2021 John Zogby… B/C         841  lv          1.12 
##  7 Joseph R.… All polls 9/14/2021 RMG Resear… B-         1200  rv          0.881
##  8 Joseph R.… All polls 9/14/2021 AP-NORC     &lt;NA&gt;       1055  a           1.43 
##  9 Joseph R.… All polls 9/14/2021 Global Str… B/C        1005  rv          0.916
## 10 Joseph R.… All polls 9/14/2021 Quinnipiac… A-         1075  a           1.53 
## # … with 1,588 more rows, and 7 more variables: influence &lt;dbl&gt;, url &lt;chr&gt;,
## #   poll_id &lt;dbl&gt;, question_id &lt;dbl&gt;, createddate &lt;chr&gt;, week &lt;dbl&gt;,
## #   net_approval &lt;dbl&gt;</code></pre>
<div id="create-a-plot" class="section level2">
<h2>Create a plot</h2>
<p>What I would like you to do is to calculate the average net approval rate (approve- disapprove) for each week since he got into office. I want you plot the net approval, along with its 95% confidence interval. There are various dates given for each poll, please use <code>enddate</code>, i.e., the date the poll ended.
Also, please add an orange line at zero. Your plot should look like this:</p>
<pre class="r"><code>formula_ci_approval &lt;- approval_weeks %&gt;% 
  
  group_by(week) %&gt;% 

  summarise(count = n(),
            mean = mean(net_approval,na.rm=TRUE),
            SD = sd(net_approval,na.rm=TRUE),
            t_critical = qt(0.975, count-1),
            SE = SD/sqrt(count),
            margin_error= t_critical*SE,
            lower = (mean - margin_error),
            upper =(mean + margin_error))

formula_ci_approval</code></pre>
<pre><code>## # A tibble: 33 × 9
##     week count  mean    SD t_critical    SE margin_error lower upper
##    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1     2    52  16.5  7.67       2.01  1.06         2.13  14.4  18.6
##  2     3    37  16.6  8.16       2.03  1.34         2.72  13.9  19.3
##  3     4    50  16.3  7.34       2.01  1.04         2.09  14.3  18.4
##  4     5    50  15.0  7.77       2.01  1.10         2.21  12.8  17.2
##  5     6    53  13.4  7.46       2.01  1.02         2.06  11.3  15.4
##  6     7    49  12.9  7.08       2.01  1.01         2.03  10.9  15.0
##  7     8    43  15.8  7.45       2.02  1.14         2.29  13.5  18.1
##  8     9    57  14.7  9.32       2.00  1.23         2.47  12.3  17.2
##  9    10    47  13.1  8.44       2.01  1.23         2.48  10.6  15.5
## 10    11    42  13.4  8.77       2.02  1.35         2.73  10.7  16.2
## # … with 23 more rows</code></pre>
<pre class="r"><code>formula_ci_approval %&gt;%
  ggplot(aes(x = week, y=mean))+
  geom_point()+
  geom_line()+
  geom_line(aes(x = week, y=upper), color = &quot;red&quot;, size=0.3)+
  geom_line(aes(x = week, y=lower), color = &quot;red&quot;, size=0.3)+
  geom_ribbon(aes(ymin=lower,ymax=upper),fill=&quot;grey&quot;, alpha=0.3)+
  geom_smooth(se = FALSE, color = &quot;blue&quot;)+
  geom_line(y=0,color = &quot;orange&quot;, size=2)+
  labs(
    title= &quot;Approval Margin for Joe Biden &quot;,
    x= &quot;Weeks&quot;,
    y= &quot;Net average approval margins &quot;
  )</code></pre>
<p><img src="/blogs/LBS2-homework2-Group7%20copia_files/figure-html/unnamed-chunk-19-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>knitr::include_graphics(here::here(&quot;images&quot;, &quot;biden_approval_margin.png&quot;), error = FALSE)</code></pre>
<p><img src="/Users/jeffreykaikati/Documents/London Business School/Applied Statistics with R/Git Repo/LBS-MAM---Jeffrey-Kaikati/images/biden_approval_margin.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="compare-confidence-intervals" class="section level2">
<h2>Compare Confidence Intervals</h2>
<p>Compare the confidence intervals for <code>week 4</code> and <code>week 25</code>. Can you explain what’s going on? One paragraph would be enough.
&gt;The CI for week 4 (in our document, recoded to week 1) is [10.42 - 19.29] while the CI for week 25 (in our document, recoded to week 21) is [7.82-11.53]. The first thing that becomes clear is that the width of the CI is different for week 1 (8.87) and 25 (3.71). The reason for this relies in the sample size of the polls: in week 1 there only were 20 polls providing data,against the 46 polls for week 25. The sample of the size influences the CIs also by affecting the SE: as we can see from the table above (formula_ci_approval) the SE week 1 is much higher than for for week 25 (2.119 vs 0.922).</p>
</div>
</div>
<div id="gapminder-revisited" class="section level1">
<h1>Gapminder revisited</h1>
<p>Recall the <code>gapminder</code> data frame from the gapminder package. That data frame contains just six columns from the larger <a href="https://www.gapminder.org/data/">data in Gapminder World</a>. In this part, you will join a few dataframes with more data than the ‘gapminder’ package. Specifically, you will look at data on</p>
<pre class="r"><code>library(gapminder)
skim(gapminder)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-20">Table 1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">gapminder</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">1704</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">6</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">factor</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">4</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: factor</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="left">ordered</th>
<th align="right">n_unique</th>
<th align="left">top_counts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">country</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">142</td>
<td align="left">Afg: 12, Alb: 12, Alg: 12, Ang: 12</td>
</tr>
<tr class="even">
<td align="left">continent</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="left">FALSE</td>
<td align="right">5</td>
<td align="left">Afr: 624, Asi: 396, Eur: 360, Ame: 300</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">year</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1.98e+03</td>
<td align="right">1.73e+01</td>
<td align="right">1952.0</td>
<td align="right">1.97e+03</td>
<td align="right">1.98e+03</td>
<td align="right">1.99e+03</td>
<td align="right">2.01e+03</td>
<td align="left">▇▅▅▅▇</td>
</tr>
<tr class="even">
<td align="left">lifeExp</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">5.95e+01</td>
<td align="right">1.29e+01</td>
<td align="right">23.6</td>
<td align="right">4.82e+01</td>
<td align="right">6.07e+01</td>
<td align="right">7.08e+01</td>
<td align="right">8.26e+01</td>
<td align="left">▁▆▇▇▇</td>
</tr>
<tr class="odd">
<td align="left">pop</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">2.96e+07</td>
<td align="right">1.06e+08</td>
<td align="right">60011.0</td>
<td align="right">2.79e+06</td>
<td align="right">7.02e+06</td>
<td align="right">1.96e+07</td>
<td align="right">1.32e+09</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">gdpPercap</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">7.22e+03</td>
<td align="right">9.86e+03</td>
<td align="right">241.2</td>
<td align="right">1.20e+03</td>
<td align="right">3.53e+03</td>
<td align="right">9.33e+03</td>
<td align="right">1.14e+05</td>
<td align="left">▇▁▁▁▁</td>
</tr>
</tbody>
</table>
<ul>
<li>Life expectancy at birth (life_expectancy_years.csv)</li>
<li>GDP per capita in constant 2010 US$ (<a href="https://data.worldbank.org/indicator/NY.GDP.PCAP.KD" class="uri">https://data.worldbank.org/indicator/NY.GDP.PCAP.KD</a>)</li>
<li>Female fertility: The number of babies per woman (<a href="https://data.worldbank.org/indicator/SP.DYN.TFRT.IN" class="uri">https://data.worldbank.org/indicator/SP.DYN.TFRT.IN</a>)</li>
<li>Primary school enrollment as % of children attending primary school (<a href="https://data.worldbank.org/indicator/SE.PRM.NENR" class="uri">https://data.worldbank.org/indicator/SE.PRM.NENR</a>)</li>
<li>Mortality rate, for under 5, per 1000 live births (<a href="https://data.worldbank.org/indicator/SH.DYN.MORT" class="uri">https://data.worldbank.org/indicator/SH.DYN.MORT</a>)</li>
<li>HIV prevalence (adults_with_hiv_percent_age_15_49.csv): The estimated number of people living with HIV per 100 population of age group 15-49.</li>
</ul>
<p>You must use the <code>wbstats</code> package to download data from the World Bank. The relevant World Bank indicators are <code>SP.DYN.TFRT.IN</code>, <code>SE.PRM.NENR</code>, <code>NY.GDP.PCAP.KD</code>, and <code>SH.DYN.MORT</code></p>
<pre class="r"><code># load gapminder HIV data
hiv &lt;- read_csv(here::here(&quot;data&quot;,&quot;adults_with_hiv_percent_age_15_49.csv&quot;))
life_expectancy &lt;- read_csv(here::here(&quot;data&quot;,&quot;life_expectancy_years.csv&quot;))

indicators &lt;- c(&quot;SP.DYN.TFRT.IN&quot;,&quot;SE.PRM.NENR&quot;, &quot;SH.DYN.MORT&quot;, &quot;NY.GDP.PCAP.KD&quot;)

library(wbstats)

worldbank_data &lt;- wb_data(country=&quot;countries_only&quot;, #countries only- no aggregates like Latin America, Europe, etc.
                          indicator = indicators, 
                          start_date = 1960, 
                          end_date = 2016)


countries &lt;-  wbstats::wb_cachelist$countries</code></pre>
<p>You have to join the 3 dataframes (life_expectancy, worldbank_data, and HIV) into one. You may need to tidy your data first and then perform <a href="http://r4ds.had.co.nz/relational-data.html">join operations</a>. Think about what type makes the most sense <strong>and explain why you chose it</strong>.</p>
<pre class="r"><code>tidy_hiv &lt;- hiv %&gt;% 
             select(1:34) %&gt;% 
            pivot_longer(cols=2:34,
             names_to=&quot;year&quot;,
             values_to = &quot;hiv&quot;)%&gt;% 
              drop_na()
tidy_hiv</code></pre>
<pre><code>## # A tibble: 3,301 × 3
##    country     year    hiv
##    &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt;
##  1 Afghanistan 2009   0.06
##  2 Afghanistan 2010   0.06
##  3 Afghanistan 2011   0.06
##  4 Algeria     1990   0.06
##  5 Algeria     1991   0.06
##  6 Algeria     1992   0.06
##  7 Algeria     1993   0.06
##  8 Algeria     1994   0.06
##  9 Algeria     1995   0.06
## 10 Algeria     1996   0.06
## # … with 3,291 more rows</code></pre>
<pre class="r"><code>skim(tidy_hiv)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-21">Table 2: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">tidy_hiv</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">3301</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">3</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">country</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">24</td>
<td align="right">0</td>
<td align="right">154</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">year</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">4</td>
<td align="right">0</td>
<td align="right">31</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">hiv</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1.74</td>
<td align="right">4.09</td>
<td align="right">0.01</td>
<td align="right">0.1</td>
<td align="right">0.3</td>
<td align="right">1.2</td>
<td align="right">26.5</td>
<td align="left">▇▁▁▁▁</td>
</tr>
</tbody>
</table>
<pre class="r"><code>tidy_life_expectancy &lt;- life_expectancy %&gt;%
    select(1:302) %&gt;%
  pivot_longer(2:302,
               names_to=&quot;year&quot;,
               values_to=&quot;life_expectancy&quot;) %&gt;%
  drop_na()

tidy_life_expectancy</code></pre>
<pre><code>## # A tibble: 55,528 × 3
##    country     year  life_expectancy
##    &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt;
##  1 Afghanistan 1800             28.2
##  2 Afghanistan 1801             28.2
##  3 Afghanistan 1802             28.2
##  4 Afghanistan 1803             28.2
##  5 Afghanistan 1804             28.2
##  6 Afghanistan 1805             28.2
##  7 Afghanistan 1806             28.1
##  8 Afghanistan 1807             28.1
##  9 Afghanistan 1808             28.1
## 10 Afghanistan 1809             28.1
## # … with 55,518 more rows</code></pre>
<pre class="r"><code>tidy_worldbank_data &lt;- worldbank_data %&gt;%
  mutate(year = as.character(date)) %&gt;%
  select(!date)
tidy_worldbank_data</code></pre>
<pre><code>## # A tibble: 12,369 × 8
##    iso2c iso3c country NY.GDP.PCAP.KD SE.PRM.NENR SH.DYN.MORT SP.DYN.TFRT.IN
##    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;
##  1 AW    ABW   Aruba               NA          NA          NA           4.82
##  2 AW    ABW   Aruba               NA          NA          NA           4.66
##  3 AW    ABW   Aruba               NA          NA          NA           4.47
##  4 AW    ABW   Aruba               NA          NA          NA           4.27
##  5 AW    ABW   Aruba               NA          NA          NA           4.06
##  6 AW    ABW   Aruba               NA          NA          NA           3.84
##  7 AW    ABW   Aruba               NA          NA          NA           3.62
##  8 AW    ABW   Aruba               NA          NA          NA           3.42
##  9 AW    ABW   Aruba               NA          NA          NA           3.23
## 10 AW    ABW   Aruba               NA          NA          NA           3.05
## # … with 12,359 more rows, and 1 more variable: year &lt;chr&gt;</code></pre>
<blockquote>
<p>We need to understand what join-function to use. Since we already cleaned the data,we want to do a left join with the left data frame being the shortest one, so that we would not have any NAs in the new dataframe.</p>
</blockquote>
<pre class="r"><code>joined_table &lt;- inner_join(left_join(left_join(tidy_hiv, tidy_worldbank_data, by = c(&quot;country&quot; = &quot;country&quot;, &quot;year&quot; = &quot;year&quot;) ), tidy_life_expectancy, by = c(&quot;country&quot; = &quot;country&quot;, &quot;year&quot; = &quot;year&quot;)), countries, &quot;country&quot; = &quot;country&quot;)


joined_table</code></pre>
<pre><code>## # A tibble: 3,117 × 25
##    country     year    hiv iso2c iso3c NY.GDP.PCAP.KD SE.PRM.NENR SH.DYN.MORT
##    &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;
##  1 Afghanistan 2009   0.06 AF    AFG             488.        NA          91.4
##  2 Afghanistan 2010   0.06 AF    AFG             543.        NA          87.6
##  3 Afghanistan 2011   0.06 AF    AFG             529.        NA          83.9
##  4 Algeria     1990   0.06 DZ    DZA            3572.        85.9        49.5
##  5 Algeria     1991   0.06 DZ    DZA            3444.        87.6        48.1
##  6 Algeria     1992   0.06 DZ    DZA            3424.        88.7        47  
##  7 Algeria     1993   0.06 DZ    DZA            3279.        88.2        45.9
##  8 Algeria     1994   0.06 DZ    DZA            3183.        88.2        44.8
##  9 Algeria     1995   0.06 DZ    DZA            3241.        88.4        43.6
## 10 Algeria     1996   0.06 DZ    DZA            3315.        87.9        42.5
## # … with 3,107 more rows, and 17 more variables: SP.DYN.TFRT.IN &lt;dbl&gt;,
## #   life_expectancy &lt;dbl&gt;, capital_city &lt;chr&gt;, longitude &lt;dbl&gt;, latitude &lt;dbl&gt;,
## #   region_iso3c &lt;chr&gt;, region_iso2c &lt;chr&gt;, region &lt;chr&gt;,
## #   admin_region_iso3c &lt;chr&gt;, admin_region_iso2c &lt;chr&gt;, admin_region &lt;chr&gt;,
## #   income_level_iso3c &lt;chr&gt;, income_level_iso2c &lt;chr&gt;, income_level &lt;chr&gt;,
## #   lending_type_iso3c &lt;chr&gt;, lending_type_iso2c &lt;chr&gt;, lending_type &lt;chr&gt;</code></pre>
<ol style="list-style-type: decimal">
<li>What is the relationship between HIV prevalence and life expectancy? Generate a scatterplot with a smoothing line to report your results. You may find faceting useful</li>
</ol>
<pre class="r"><code>joined_table %&gt;%
  ggplot(aes(x=hiv,y=life_expectancy))+
  geom_point()+
  geom_smooth(method=&quot;lm&quot;)+
  facet_wrap(~region)+
  labs(
    title= &quot;Relationship between life expectancy and Hiv diffusion&quot;,
    x= &quot;Hiv diffusion, (% population)&quot;,
    y =&quot;Life expectancy, years&quot;
  )</code></pre>
<p><img src="/blogs/LBS2-homework2-Group7%20copia_files/figure-html/unnamed-chunk-25-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>joined_table %&gt;%
  group_by(region) %&gt;%
  summarise(count = n())</code></pre>
<pre><code>## # A tibble: 7 × 2
##   region                     count
##   &lt;chr&gt;                      &lt;int&gt;
## 1 East Asia &amp; Pacific          316
## 2 Europe &amp; Central Asia        918
## 3 Latin America &amp; Caribbean    567
## 4 Middle East &amp; North Africa   192
## 5 North America                 46
## 6 South Asia                   156
## 7 Sub-Saharan Africa           922</code></pre>
<blockquote>
<p>The relationship between the two variables shows a negative correlation for each region but for Europe&amp;Centralise Asia: the more HIV is diffused in the region, the lower the life expectancy.
We checked the data size for the region to try to understand why. In Europe we have 918 data points(the second highest data point per region).We assume say in Europe, people that have been diagnosed by Hiv probably by getting the medication are able to live longer. Very surprising are also the results for Sub-Saharan Africa, where we find a wide range of diffusion of HIV, showing that the diffusion of the illness is very different for each country. Interesting as well is the fact that having HIV in the Middle East and North Africa highly reduces the life expectancy.</p>
</blockquote>
<ol start="2" style="list-style-type: decimal">
<li>What is the relationship between fertility rate and GDP per capita? Generate a scatterplot with a smoothing line to report your results. You may find facetting by region useful</li>
</ol>
<pre class="r"><code>joined_table %&gt;%
  ggplot(aes(x=NY.GDP.PCAP.KD,y=SP.DYN.TFRT.IN))+
  geom_point()+
  facet_wrap(~region)+
  geom_smooth(method = &quot;lm&quot;)+
  labs(
    title=&quot;Relationship between fertility rate and GDP per capita&quot;,
    y=&quot;Fertility rate, kids/women&quot;,
    x= &quot;GDP, constant 2010 US$&quot;
  )</code></pre>
<p><img src="/blogs/LBS2-homework2-Group7%20copia_files/figure-html/unnamed-chunk-26-1.png" width="648" style="display: block; margin: auto;" />
&gt;We do not see a relationship for Europe&amp;Central America and for Middle East &amp; North Africa. However, we do find that the higher the GDP the lower the fertility rate for all other regions.</p>
<ol start="3" style="list-style-type: decimal">
<li>Which regions have the most observations with missing HIV data? Generate a bar chart (<code>geom_col()</code>), in descending order.</li>
</ol>
<pre class="r"><code>hiv_na &lt;- hiv %&gt;% 
             select(1:34) %&gt;% 
            pivot_longer(cols=2:34,
             names_to=&quot;year&quot;,
             values_to = &quot;hiv&quot;)
hiv_na_country &lt;- left_join(hiv_na, countries, &quot;country&quot;= &quot;country&quot;)
              
hiv_na_country</code></pre>
<pre><code>## # A tibble: 5,082 × 20
##    country     year    hiv iso3c iso2c capital_city longitude latitude region_iso3c
##    &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       
##  1 Afghanistan 1979     NA AFG   AF    Kabul             69.2     34.5 SAS         
##  2 Afghanistan 1980     NA AFG   AF    Kabul             69.2     34.5 SAS         
##  3 Afghanistan 1981     NA AFG   AF    Kabul             69.2     34.5 SAS         
##  4 Afghanistan 1982     NA AFG   AF    Kabul             69.2     34.5 SAS         
##  5 Afghanistan 1983     NA AFG   AF    Kabul             69.2     34.5 SAS         
##  6 Afghanistan 1984     NA AFG   AF    Kabul             69.2     34.5 SAS         
##  7 Afghanistan 1985     NA AFG   AF    Kabul             69.2     34.5 SAS         
##  8 Afghanistan 1986     NA AFG   AF    Kabul             69.2     34.5 SAS         
##  9 Afghanistan 1987     NA AFG   AF    Kabul             69.2     34.5 SAS         
## 10 Afghanistan 1988     NA AFG   AF    Kabul             69.2     34.5 SAS         
## # … with 5,072 more rows, and 11 more variables: region_iso2c &lt;chr&gt;,
## #   region &lt;chr&gt;, admin_region_iso3c &lt;chr&gt;, admin_region_iso2c &lt;chr&gt;,
## #   admin_region &lt;chr&gt;, income_level_iso3c &lt;chr&gt;, income_level_iso2c &lt;chr&gt;,
## #   income_level &lt;chr&gt;, lending_type_iso3c &lt;chr&gt;, lending_type_iso2c &lt;chr&gt;,
## #   lending_type &lt;chr&gt;</code></pre>
<pre class="r"><code>hiv_na_country %&gt;%
  filter(!is.na(region)) %&gt;%
  group_by(region) %&gt;%
  summarise(na=sum(is.na(hiv)))%&gt;%
  mutate(region=fct_reorder(region, na))%&gt;%
  ggplot(aes(x=na,y=region))+
  geom_col()+
  labs(
    title=&quot;Number of missing HIV observations per region&quot;,
    y=&quot;&quot;,
    x= &quot;Number of NAs values&quot;
  )</code></pre>
<p><img src="/blogs/LBS2-homework2-Group7%20copia_files/figure-html/unnamed-chunk-27-1.png" width="648" style="display: block; margin: auto;" />
&gt;Sub-Saharan Africa and Europe&amp; Central Asia have the highest number of observations with missing HIV data.</p>
<ol start="4" style="list-style-type: decimal">
<li>How has mortality rate for under 5 changed by region? In each region, find the top 5 countries that have seen the greatest improvement, as well as those 5 countries where mortality rates have had the least improvement or even deterioration.</li>
</ol>
<pre class="r"><code>endpoints &lt;- joined_table %&gt;%
  group_by(country)%&gt;%
  summarise(min_year=min(year), max_year=max(year))</code></pre>
<pre class="r"><code>endpoints</code></pre>
<pre><code>## # A tibble: 143 × 3
##    country     min_year max_year
##    &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;   
##  1 Afghanistan 2009     2011    
##  2 Algeria     1990     2008    
##  3 Angola      1979     2011    
##  4 Argentina   1979     2011    
##  5 Armenia     1990     2011    
##  6 Australia   1983     2011    
##  7 Austria     1981     2011    
##  8 Azerbaijan  1990     2011    
##  9 Bangladesh  1990     2011    
## 10 Barbados    1979     2011    
## # … with 133 more rows</code></pre>
<pre class="r"><code>endpoints &lt;- joined_table %&gt;%
  group_by(country) %&gt;%
  summarise(min_year = min(year), max_year = max(year))

endpoints</code></pre>
<pre><code>## # A tibble: 143 × 3
##    country     min_year max_year
##    &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;   
##  1 Afghanistan 2009     2011    
##  2 Algeria     1990     2008    
##  3 Angola      1979     2011    
##  4 Argentina   1979     2011    
##  5 Armenia     1990     2011    
##  6 Australia   1983     2011    
##  7 Austria     1981     2011    
##  8 Azerbaijan  1990     2011    
##  9 Bangladesh  1990     2011    
## 10 Barbados    1979     2011    
## # … with 133 more rows</code></pre>
<pre class="r"><code>joiner_min &lt;- left_join(joined_table, endpoints, &quot;country&quot;=&quot;country&quot;) %&gt;%
  select(country, year, min_year, max_year, SH.DYN.MORT)%&gt;%
  mutate(mortality_origin = ifelse(year == min_year, SH.DYN.MORT, 0))%&gt;%
  select(!year)%&gt;%
  filter(!mortality_origin==0)%&gt;%
  select(!SH.DYN.MORT)

joiner_min</code></pre>
<pre><code>## # A tibble: 139 × 4
##    country     min_year max_year mortality_origin
##    &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;               &lt;dbl&gt;
##  1 Afghanistan 2009     2011                 91.4
##  2 Algeria     1990     2008                 49.5
##  3 Argentina   1979     2011                 46.8
##  4 Armenia     1990     2011                 48.9
##  5 Australia   1983     2011                 11.9
##  6 Austria     1981     2011                 15.5
##  7 Azerbaijan  1990     2011                 95.2
##  8 Bangladesh  1990     2011                144. 
##  9 Barbados    1979     2011                 28.6
## 10 Belarus     1990     2011                 15.2
## # … with 129 more rows</code></pre>
<pre class="r"><code>joiner_max &lt;- left_join(joined_table, endpoints, &quot;country&quot;=&quot;country&quot;) %&gt;%
  select(country, year, min_year, max_year, SH.DYN.MORT)%&gt;%
  mutate(mortality_end = ifelse(year == max_year, SH.DYN.MORT, 0))%&gt;%
  select(!year)%&gt;%
  filter(!mortality_end==0)%&gt;%
  select(!SH.DYN.MORT)

joiner_max</code></pre>
<pre><code>## # A tibble: 143 × 4
##    country     min_year max_year mortality_end
##    &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;            &lt;dbl&gt;
##  1 Afghanistan 2009     2011              83.9
##  2 Algeria     1990     2008              29.5
##  3 Angola      1979     2011             112. 
##  4 Argentina   1979     2011              13.9
##  5 Armenia     1990     2011              17.6
##  6 Australia   1983     2011               4.5
##  7 Austria     1981     2011               4.2
##  8 Azerbaijan  1990     2011              35  
##  9 Bangladesh  1990     2011              46.1
## 10 Barbados    1979     2011              14.9
## # … with 133 more rows</code></pre>
<pre class="r"><code>join_joiners &lt;- left_join(joiner_min, joiner_max, &quot;country&quot;=&quot;country&quot;)
join_joiners</code></pre>
<pre><code>## # A tibble: 139 × 5
##    country     min_year max_year mortality_origin mortality_end
##    &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;
##  1 Afghanistan 2009     2011                 91.4          83.9
##  2 Algeria     1990     2008                 49.5          29.5
##  3 Argentina   1979     2011                 46.8          13.9
##  4 Armenia     1990     2011                 48.9          17.6
##  5 Australia   1983     2011                 11.9           4.5
##  6 Austria     1981     2011                 15.5           4.2
##  7 Azerbaijan  1990     2011                 95.2          35  
##  8 Bangladesh  1990     2011                144.           46.1
##  9 Barbados    1979     2011                 28.6          14.9
## 10 Belarus     1990     2011                 15.2           5.1
## # … with 129 more rows</code></pre>
<pre class="r"><code>by_region &lt;- left_join(join_joiners, countries, &quot;country&quot;=&quot;country&quot;) %&gt;%
  select(country,region,min_year,max_year,mortality_origin,mortality_end)
by_region</code></pre>
<pre><code>## # A tibble: 139 × 6
##    country     region           min_year max_year mortality_origin mortality_end
##    &lt;chr&gt;       &lt;chr&gt;            &lt;chr&gt;    &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;
##  1 Afghanistan South Asia       2009     2011                 91.4          83.9
##  2 Algeria     Middle East &amp; N… 1990     2008                 49.5          29.5
##  3 Argentina   Latin America &amp;… 1979     2011                 46.8          13.9
##  4 Armenia     Europe &amp; Centra… 1990     2011                 48.9          17.6
##  5 Australia   East Asia &amp; Pac… 1983     2011                 11.9           4.5
##  6 Austria     Europe &amp; Centra… 1981     2011                 15.5           4.2
##  7 Azerbaijan  Europe &amp; Centra… 1990     2011                 95.2          35  
##  8 Bangladesh  South Asia       1990     2011                144.           46.1
##  9 Barbados    Latin America &amp;… 1979     2011                 28.6          14.9
## 10 Belarus     Europe &amp; Centra… 1990     2011                 15.2           5.1
## # … with 129 more rows</code></pre>
<pre class="r"><code>region_list = c(&quot;East Asia &amp; Pacific&quot;   ,&quot;Europe &amp; Central Asia&quot;    ,&quot;Latin America &amp; Caribbean&quot;,&quot;Middle East &amp; North Africa&quot;,&quot;North America&quot;,&quot;South Asia&quot;,&quot;Sub-Saharan Africa&quot;)


prepared_data &lt;- left_join(join_joiners, countries, &quot;country&quot;=&quot;country&quot;) %&gt;%
  
  mutate(min_year = as.numeric(min_year),max_year = as.numeric(max_year),mortality_origin = as.numeric(mortality_origin),mortality_end = as.numeric(mortality_end),evolution_of_mortality = (mortality_end-mortality_origin)/mortality_origin)%&gt;%
  
  arrange(desc(evolution_of_mortality))%&gt;%
  
  select(country,region,min_year,max_year,mortality_origin,mortality_end, evolution_of_mortality)</code></pre>
<blockquote>
<p>The first table depicts the the top 5 countries that have seen the greatest improvement; the second one the top 5 countries where mortality rates have had the least improvement
East Asia &amp; Pacific</p>
</blockquote>
<pre class="r"><code>prepared_data %&gt;%
  filter(region == region_list[1])%&gt;%
  slice_max(order_by = evolution_of_mortality,n = 5)</code></pre>
<pre><code>## # A tibble: 5 × 7
##   country          region              min_year max_year mortality_origin mortality_end
##   &lt;chr&gt;            &lt;chr&gt;                  &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;
## 1 China            East Asia &amp; Pacific     2011     2011             14.6          14.6
## 2 Fiji             East Asia &amp; Pacific     1990     2011             28.9          23.6
## 3 Papua New Guinea East Asia &amp; Pacific     1987     2011             89.9          56  
## 4 Philippines      East Asia &amp; Pacific     1990     2011             56.6          31.3
## 5 Japan            East Asia &amp; Pacific     1990     2011              6.3           3.2
## # … with 1 more variable: evolution_of_mortality &lt;dbl&gt;</code></pre>
<pre class="r"><code>prepared_data %&gt;%
  filter(region == region_list[1])%&gt;%
  slice_max(order_by = -evolution_of_mortality,n = 5)</code></pre>
<pre><code>## # A tibble: 5 × 7
##   country   region              min_year max_year mortality_origin mortality_end
##   &lt;chr&gt;     &lt;chr&gt;                  &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;
## 1 Mongolia  East Asia &amp; Pacific     1990     2011            108.           27.9
## 2 Thailand  East Asia &amp; Pacific     1985     2011             47.8          13  
## 3 Cambodia  East Asia &amp; Pacific     1985     2011            120.           40.6
## 4 Singapore East Asia &amp; Pacific     1990     2011              7.7           2.8
## 5 Australia East Asia &amp; Pacific     1983     2011             11.9           4.5
## # … with 1 more variable: evolution_of_mortality &lt;dbl&gt;</code></pre>
<blockquote>
<p>Europe &amp; Central Asia</p>
</blockquote>
<pre class="r"><code>prepared_data %&gt;%
  filter(region == region_list[2])%&gt;%
  slice_max(order_by = evolution_of_mortality,n = 5)</code></pre>
<pre><code>## # A tibble: 5 × 7
##   country     region                min_year max_year mortality_origin mortality_end
##   &lt;chr&gt;       &lt;chr&gt;                    &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;
## 1 Ukraine     Europe &amp; Central Asia     1990     2011             19.4          11.2
## 2 Bulgaria    Europe &amp; Central Asia     1990     2011             18.4          10.4
## 3 Uzbekistan  Europe &amp; Central Asia     1990     2008             72.3          38.5
## 4 Moldova     Europe &amp; Central Asia     1990     2011             33.2          16.8
## 5 Switzerland Europe &amp; Central Asia     1985     2011              9             4.5
## # … with 1 more variable: evolution_of_mortality &lt;dbl&gt;</code></pre>
<pre class="r"><code>prepared_data %&gt;%
  filter(region == region_list[2])%&gt;%
  slice_max(order_by = -evolution_of_mortality,n = 5)</code></pre>
<pre><code>## # A tibble: 5 × 7
##   country    region                min_year max_year mortality_origin mortality_end
##   &lt;chr&gt;      &lt;chr&gt;                    &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;
## 1 Portugal   Europe &amp; Central Asia     1979     2011             29.7           3.7
## 2 Greece     Europe &amp; Central Asia     1979     2011             21             3.9
## 3 Serbia     Europe &amp; Central Asia     1985     2011             39.1           7.4
## 4 Luxembourg Europe &amp; Central Asia     1979     2011             13.9           3  
## 5 Italy      Europe &amp; Central Asia     1979     2011             17             3.9
## # … with 1 more variable: evolution_of_mortality &lt;dbl&gt;</code></pre>
<blockquote>
<p>Latin America &amp; Caribbean</p>
</blockquote>
<pre class="r"><code>prepared_data %&gt;%
  filter(region == region_list[3])%&gt;%
  slice_max(order_by = evolution_of_mortality,n = 5)</code></pre>
<pre><code>## # A tibble: 5 × 7
##   country             region    min_year max_year mortality_origin mortality_end
##   &lt;chr&gt;               &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;
## 1 Costa Rica          Latin Am…     1990     2011             16.9          10.2
## 2 Trinidad and Tobago Latin Am…     1982     2011             36.9          22.2
## 3 Barbados            Latin Am…     1979     2011             28.6          14.9
## 4 Guyana              Latin Am…     1979     2011             70.5          36.5
## 5 Belize              Latin Am…     1990     2011             38.3          18.3
## # … with 1 more variable: evolution_of_mortality &lt;dbl&gt;</code></pre>
<pre class="r"><code>prepared_data %&gt;%
  filter(region == region_list[3])%&gt;%
  slice_max(order_by = -evolution_of_mortality,n = 5)</code></pre>
<pre><code>## # A tibble: 5 × 7
##   country     region            min_year max_year mortality_origin mortality_end
##   &lt;chr&gt;       &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;
## 1 Peru        Latin America &amp; …     1979     2011            129            18.9
## 2 Brazil      Latin America &amp; …     1979     2011            100.           17.9
## 3 Ecuador     Latin America &amp; …     1979     2011             96.1          17.5
## 4 El Salvador Latin America &amp; …     1985     2011             79.8          18.3
## 5 Uruguay     Latin America &amp; …     1979     2011             44            10.3
## # … with 1 more variable: evolution_of_mortality &lt;dbl&gt;</code></pre>
<blockquote>
<p>Middle East &amp; North Africa</p>
</blockquote>
<pre class="r"><code>prepared_data %&gt;%
  filter(region == region_list[4])%&gt;%
  slice_max(order_by = evolution_of_mortality,n = 5)</code></pre>
<pre><code>## # A tibble: 5 × 7
##   country  region                     min_year max_year mortality_origin mortality_end
##   &lt;chr&gt;    &lt;chr&gt;                         &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;
## 1 Malta    Middle East &amp; North Africa     1990     2011             11.4           6.8
## 2 Algeria  Middle East &amp; North Africa     1990     2008             49.5          29.5
## 3 Djibouti Middle East &amp; North Africa     1983     2011            142.           74.1
## 4 Qatar    Middle East &amp; North Africa     1990     2008             20.8           9.6
## 5 Morocco  Middle East &amp; North Africa     1990     2011             79.1          30.4
## # … with 1 more variable: evolution_of_mortality &lt;dbl&gt;</code></pre>
<pre class="r"><code>prepared_data %&gt;%
  filter(region == region_list[4])%&gt;%
  slice_max(order_by = -evolution_of_mortality,n = 5)</code></pre>
<pre><code>## # A tibble: 5 × 7
##   country region                     min_year max_year mortality_origin mortality_end
##   &lt;chr&gt;   &lt;chr&gt;                         &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;
## 1 Lebanon Middle East &amp; North Africa     1979     2011             51.5           9.8
## 2 Israel  Middle East &amp; North Africa     1979     2011             19.1           4.4
## 3 Oman    Middle East &amp; North Africa     1990     2008             38.7          12  
## 4 Tunisia Middle East &amp; North Africa     1990     2011             55.3          18  
## 5 Morocco Middle East &amp; North Africa     1990     2011             79.1          30.4
## # … with 1 more variable: evolution_of_mortality &lt;dbl&gt;</code></pre>
<blockquote>
<p>North America</p>
</blockquote>
<pre class="r"><code>prepared_data %&gt;%
  filter(region == region_list[5])%&gt;%
  slice_max(order_by = evolution_of_mortality,n = 5)</code></pre>
<pre><code>## # A tibble: 2 × 7
##   country       region        min_year max_year mortality_origin mortality_end
##   &lt;chr&gt;         &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;
## 1 United States North America     1979     2011             15.6           7.2
## 2 Canada        North America     1979     2011             13.3           5.6
## # … with 1 more variable: evolution_of_mortality &lt;dbl&gt;</code></pre>
<pre class="r"><code>prepared_data %&gt;%
  filter(region == region_list[5])%&gt;%
  slice_max(order_by = -evolution_of_mortality,n = 5)</code></pre>
<pre><code>## # A tibble: 2 × 7
##   country       region        min_year max_year mortality_origin mortality_end
##   &lt;chr&gt;         &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;
## 1 Canada        North America     1979     2011             13.3           5.6
## 2 United States North America     1979     2011             15.6           7.2
## # … with 1 more variable: evolution_of_mortality &lt;dbl&gt;</code></pre>
<blockquote>
<p>South Asia</p>
</blockquote>
<pre class="r"><code>prepared_data %&gt;%
  filter(region == region_list[6])%&gt;%
  slice_max(order_by = evolution_of_mortality,n = 5)</code></pre>
<pre><code>## # A tibble: 5 × 7
##   country     region     min_year max_year mortality_origin mortality_end
##   &lt;chr&gt;       &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;
## 1 Afghanistan South Asia     2009     2011             91.4          83.9
## 2 Pakistan    South Asia     1990     2011            140.           85  
## 3 Sri Lanka   South Asia     1990     2011             21.9          11.2
## 4 India       South Asia     1990     2009            126.           61.4
## 5 Bangladesh  South Asia     1990     2011            144.           46.1
## # … with 1 more variable: evolution_of_mortality &lt;dbl&gt;</code></pre>
<pre class="r"><code>prepared_data %&gt;%
  filter(region == region_list[6])%&gt;%
  slice_max(order_by = -evolution_of_mortality,n = 5)</code></pre>
<pre><code>## # A tibble: 5 × 7
##   country    region     min_year max_year mortality_origin mortality_end
##   &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;
## 1 Maldives   South Asia     1990     2011             85.8          13  
## 2 Nepal      South Asia     1985     2011            175.           44.2
## 3 Bhutan     South Asia     1990     2011            127            39.9
## 4 Bangladesh South Asia     1990     2011            144.           46.1
## 5 India      South Asia     1990     2009            126.           61.4
## # … with 1 more variable: evolution_of_mortality &lt;dbl&gt;</code></pre>
<blockquote>
<p>Sub-Saharan Africa</p>
</blockquote>
<pre class="r"><code>prepared_data %&gt;%
  filter(region == region_list[7])%&gt;%
  slice_max(order_by = evolution_of_mortality,n = 5)</code></pre>
<pre><code>## # A tibble: 5 × 7
##   country               region  min_year max_year mortality_origin mortality_end
##   &lt;chr&gt;                 &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;
## 1 South Sudan           Sub-Sa…     2011     2011            102.          102. 
## 2 Lesotho               Sub-Sa…     1984     2011            101.           96.1
## 3 Ethiopia              Sub-Sa…     2009     2011             86.7          77.6
## 4 Sao Tome and Principe Sub-Sa…     2009     2011             47.9          42.4
## 5 Somalia               Sub-Sa…     1987     2011            186           151. 
## # … with 1 more variable: evolution_of_mortality &lt;dbl&gt;</code></pre>
<pre class="r"><code>prepared_data %&gt;%
  filter(region == region_list[7])%&gt;%
  slice_max(order_by = -evolution_of_mortality,n = 5)</code></pre>
<pre><code>## # A tibble: 5 × 7
##   country region             min_year max_year mortality_origin mortality_end
##   &lt;chr&gt;   &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;
## 1 Rwanda  Sub-Saharan Africa     1979     2011             233.          57.1
## 2 Eritrea Sub-Saharan Africa     1985     2011             178.          53.2
## 3 Malawi  Sub-Saharan Africa     1980     2011             256.          78.6
## 4 Senegal Sub-Saharan Africa     1981     2011             198.          63  
## 5 Uganda  Sub-Saharan Africa     1979     2011             216.          72.5
## # … with 1 more variable: evolution_of_mortality &lt;dbl&gt;</code></pre>
<pre class="r"><code>evolution_included &lt;- by_region %&gt;%
  mutate(min_year = as.numeric(min_year),max_year = as.numeric(max_year),mortality_origin = as.numeric(mortality_origin),mortality_end = as.numeric(mortality_end),evolution_of_mortality = (mortality_end-mortality_origin)/mortality_origin)%&gt;%
  arrange(desc(evolution_of_mortality))

evolution_included</code></pre>
<pre><code>## # A tibble: 139 × 7
##    country                  region min_year max_year mortality_origin mortality_end
##    &lt;chr&gt;                    &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;
##  1 China                    East …     2011     2011             14.6          14.6
##  2 South Sudan              Sub-S…     2011     2011            102.          102. 
##  3 Lesotho                  Sub-S…     1984     2011            101.           96.1
##  4 Afghanistan              South…     2009     2011             91.4          83.9
##  5 Ethiopia                 Sub-S…     2009     2011             86.7          77.6
##  6 Sao Tome and Principe    Sub-S…     2009     2011             47.9          42.4
##  7 Fiji                     East …     1990     2011             28.9          23.6
##  8 Somalia                  Sub-S…     1987     2011            186           151. 
##  9 Mauritania               Sub-S…     1990     2011            118.           93.7
## 10 Central African Republic Sub-S…     1979     2011            188.          143. 
## # … with 129 more rows, and 1 more variable: evolution_of_mortality &lt;dbl&gt;</code></pre>
<pre class="r"><code>evolution_by_region &lt;- evolution_included %&gt;%
  group_by(region) %&gt;%
  summarise(mean_mortality_evol = mean(evolution_of_mortality))

evolution_by_region</code></pre>
<pre><code>## # A tibble: 7 × 2
##   region                     mean_mortality_evol
##   &lt;chr&gt;                                    &lt;dbl&gt;
## 1 East Asia &amp; Pacific                     -0.514
## 2 Europe &amp; Central Asia                   -0.651
## 3 Latin America &amp; Caribbean               -0.633
## 4 Middle East &amp; North Africa              -0.598
## 5 North America                           -0.559
## 6 South Asia                              -0.555
## 7 Sub-Saharan Africa                      -0.455</code></pre>
<pre class="r"><code>evolution_by_region %&gt;%
  ggplot(aes(y = reorder(region, --mean_mortality_evol), x=100*mean_mortality_evol))+
  geom_col()+
  labs(
    title= &quot;Change in mortality rate by region&quot;,
    x= &quot;Change in mortality rate, (%)&quot;,
    y=&quot;&quot;
  )</code></pre>
<p><img src="/blogs/LBS2-homework2-Group7%20copia_files/figure-html/unnamed-chunk-43-1.png" width="648" style="display: block; margin: auto;" /></p>
<ol start="5" style="list-style-type: decimal">
<li>Is there a relationship between primary school enrollment and fertility rate?</li>
</ol>
<pre class="r"><code>#skim(joined_table)


joined_table %&gt;%
  group_by(country) %&gt;%
  ggplot(aes(x=SE.PRM.NENR,y=SP.DYN.TFRT.IN))+
  geom_smooth(se=FALSE, na.rm=TRUE)+
  theme(axis.title = element_text())+
  facet_wrap(~region)+
  labs (
    title = &quot;Primary school enrollment and fertility rate by region&quot;,
    y= &quot;Fertility rate&quot;,
    x= &quot;Primary school enrollment, (%) population&quot;
  )</code></pre>
<p><img src="/blogs/LBS2-homework2-Group7%20copia_files/figure-html/unnamed-chunk-44-1.png" width="648" style="display: block; margin: auto;" /></p>
<pre class="r"><code>joined_table %&gt;%
  group_by(country) %&gt;%
  ggplot(aes(x=SE.PRM.NENR,y=SP.DYN.TFRT.IN))+
  geom_smooth(se=FALSE, na.rm=TRUE)+
  theme(axis.title = element_text())+
  labs (
    title = &quot;Decreasing fertility rate with increased primary school enrollment&quot;,
    y= &quot;Fertility rate&quot;,
    x= &quot;Primary school enrollment, (%) population&quot;
  )</code></pre>
<p><img src="/blogs/LBS2-homework2-Group7%20copia_files/figure-html/unnamed-chunk-44-2.png" width="648" style="display: block; margin: auto;" /></p>
<blockquote>
<p>Overall, we recognise a clear negativ correlation between the two variables: the higher the primary school enrollment, the lower the fertility rate. Looking at the differences between regions, it becomes clear that the decrease in fertility rate has been more sharp for some regions than for others: In Sub-Saharan Africa and Latin america &amp; Caribbean, fertility rate decreased more than it did in Europe&amp;Central Asia, region in which primary school enrollment has been very high in the analysed timeframe (1960-2016).</p>
</blockquote>
</div>
<div id="challenge-1-excess-rentals-in-tfl-bike-sharing" class="section level1">
<h1>Challenge 1: Excess rentals in TfL bike sharing</h1>
<p>Recall the TfL data on how many bikes were hired every single day. We can get the latest data by running the following</p>
<pre class="r"><code>url &lt;- &quot;https://data.london.gov.uk/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xlsx&quot;
library(disk.frame)
# Download TFL data to temporary file
httr::GET(url, write_disk(bike.temp &lt;- tempfile(fileext = &quot;.xlsx&quot;)))</code></pre>
<pre><code>## Response [https://airdrive-secure.s3-eu-west-1.amazonaws.com/london/dataset/number-bicycle-hires/2021-08-23T14%3A32%3A29/tfl-daily-cycle-hires.xlsx?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAJJDIMAIVZJDICKHA%2F20210914%2Feu-west-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20210914T161653Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=afd054a8f6602b3f930fd8b89854b202a5d99905a80be75c504fd3b9690930f9&amp;X-Amz-SignedHeaders=host]
##   Date: 2021-09-14 16:16
##   Status: 200
##   Content-Type: application/vnd.openxmlformats-officedocument.spreadsheetml.sheet
##   Size: 173 kB
## &lt;ON DISK&gt;  /var/folders/9l/m55q3p1j0z5b8bqn0kvk02b00000gn/T//Rtmp0mGGsR/file28a95f945685.xlsx</code></pre>
<pre class="r"><code># Use read_excel to read it as dataframe
bike0 &lt;- read_excel(bike.temp,
                   sheet = &quot;Data&quot;,
                   range = cell_cols(&quot;A:B&quot;))

# change dates to get year, month, and week
bike &lt;- bike0 %&gt;% 
  clean_names() %&gt;% 
  rename (bikes_hired = number_of_bicycle_hires) %&gt;% 
  mutate (year = year(day),
          month = lubridate::month(day, label = TRUE),
          week = isoweek(day))</code></pre>
<p>We can easily create a facet grid that plots bikes hired by month and year.</p>
<pre class="r"><code>knitr::include_graphics(here::here(&quot;images&quot;, &quot;tfl_distributions_monthly.png&quot;), error = FALSE)</code></pre>
<p><img src="/Users/jeffreykaikati/Documents/London Business School/Applied Statistics with R/Git Repo/LBS-MAM---Jeffrey-Kaikati/images/tfl_distributions_monthly.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Look at May and Jun and compare 2020 with the previous years. What’s happening?</p>
<blockquote>
<p>It becomes clear that the effect of national restrictive measures such as lockdowns have greatly influenced the bikes rent. This helps us explain the difference between the average number of rented bikes in May and June 2020 with previous years.</p>
</blockquote>
<pre class="r"><code>actual_bike &lt;- bike %&gt;%
               filter (year &gt;= 2016) %&gt;%
               group_by(year,month) %&gt;%
               summarise(actual = mean(bikes_hired))

expected_bike &lt;- actual_bike %&gt;%
                 group_by(month) %&gt;%
                 summarise(expected = mean(actual))

comparison_bike &lt;- left_join(actual_bike, expected_bike, by = &quot;month&quot;)

comparison_bike</code></pre>
<pre><code>## # A tibble: 67 × 4
## # Groups:   year [6]
##     year month actual expected
##    &lt;dbl&gt; &lt;ord&gt;  &lt;dbl&gt;    &lt;dbl&gt;
##  1  2016 Jan   18914.   19763.
##  2  2016 Feb   20608.   21433.
##  3  2016 Mar   21435    22491.
##  4  2016 Apr   25444.   27392.
##  5  2016 May   32699.   33163.
##  6  2016 Jun   32108.   36618.
##  7  2016 Jul   38336.   37974.
##  8  2016 Aug   37368.   34955.
##  9  2016 Sep   35101.   33994.
## 10  2016 Oct   30488.   29660.
## # … with 57 more rows</code></pre>
<pre class="r"><code>comparison_bike %&gt;%
  ggplot(aes(x = month, group = 1)) +
  geom_line(aes(x = month, y = actual), color = &quot;black&quot;, size = 0.1) +
  geom_line(aes(x = month, y = expected), color = &quot;blue&quot;, size = 0.8) +
  geom_ribbon(aes(ymin = expected, ymax = pmin(expected, actual)),fill = &quot;red&quot;, alpha=0.2)  +
  geom_ribbon(aes(ymin = actual, ymax = pmin(expected, actual)),fill = &quot;green&quot;, alpha=0.2)+
  facet_wrap(~ year) +
  theme_bw()+
  labs(
    title= &quot;Montly changes in Tfl bikes rentals&quot;,
    y=&quot;bike rentals&quot;,
    x=&quot;Months&quot;
  )</code></pre>
<p><img src="/blogs/LBS2-homework2-Group7%20copia_files/figure-html/tfl_absolute_monthly_change-1.png" width="100%" style="display: block; margin: auto;" /></p>
<pre class="r"><code>actual_bike_w &lt;- bike %&gt;%
               filter (year &gt;= 2016) %&gt;%
               group_by(year, week) %&gt;%
               summarise(actual = mean(bikes_hired))

expected_bike_w &lt;- actual_bike_w %&gt;%
                 group_by(week) %&gt;%
                 summarise(expected = mean(actual))

comparison_bike_w &lt;- left_join(actual_bike_w, expected_bike_w, by = &quot;week&quot;) %&gt;%
                     group_by(week) %&gt;%
                     mutate(dchanges = (actual - expected) / expected )

comparison_bike_w = comparison_bike_w %&gt;%
  filter(!(year ==2021 &amp; week ==53))</code></pre>
<pre class="r"><code>comparison_bike_w %&gt;%
  ggplot(aes(x = week, group = 1)) +
  geom_line(aes(x = week, y = dchanges, fill = &quot;black&quot;)) +
  geom_ribbon(aes(ymin = 0, ymax = pmin(0, dchanges)),fill = &quot;red&quot;, alpha=0.2)  +
  geom_ribbon(aes(ymin = dchanges, ymax = pmin(0, dchanges)),fill = &quot;green&quot;, alpha=0.2)+
  facet_wrap(~ year) +
  theme_bw()+
  labs(
    title= &quot;Weekly changes in Tfl bikes rentals&quot;,
    y= &quot;Bikes rentals&quot;,
    x=&quot;Weeks&quot;
  )</code></pre>
<p><img src="/blogs/LBS2-homework2-Group7%20copia_files/figure-html/tfl_absolute_monthly_changemj-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Should you use the mean or the median to calculate your expected rentals? Why?
&gt; In order to calculate the expected rentals we used the mean of rented bikes/montly since we thought this was a better measurement. Since the monthly data of the actual rented bikes does not seem to be heavily right/left skewed, the mean is a good tool to calcukate the expected rentals. If the data were heavily skewed, we would have changed to the median.</p>
</div>
<div id="deliverables" class="section level1">
<h1>Deliverables</h1>
<p>As usual, there is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed R Markdown file as an HTML document (use the “Knit” button at the top of the script editor window) and upload it to Canvas.</p>
</div>
<div id="details" class="section level1">
<h1>Details</h1>
<ul>
<li>Who did you collaborate with: Awashti Palak, Kaikati Jeffrey, Laffitte Jose, Opre Valeria, Wang Hanyu, Zhang Jasmine</li>
<li>Approximately how much time did you spend on this problem set: Monday 2h, Wednesday 2h , Thursday 7h, Friday 2h - total of13h</li>
<li>What, if anything, gave you the most trouble: Defining the mortality rate has been a challenge for us, and also having to join the dataframes by two variables (countries and years)</li>
</ul>
</div>
