---
categories:
- ""
- ""
date: "2017-10-31T22:42:51-05:00"
description: HOMEWORK 1
draft: false
image: pic07.jpg
keywords: ""
slug: homework1
title: Homework 1
---

---
title: 'Session 2: Homework 1'
author: "Awashti Palak, Kaikati Jeffrey, Laffitte Jose, Opre Valeria, Wang Hanyu, Zhang Jasmine "
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---

```{r}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(fivethirtyeight)
library(here)
library(skimr)
library(janitor)
library(vroom)
library(tidyquant)
library(rvest) # to scrape wikipedia page
```



# Where Do People Drink The Most Beer, Wine And Spirits?

Back in 2014, [fivethiryeight.com](https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/) published an article on alchohol consumption in different countries. The data `drinks` is available as part of the `fivethirtyeight` package. Make sure you have installed the `fivethirtyeight` package before proceeding.


```{r, load_alcohol_data}
library(fivethirtyeight)
data(drinks)


# or download directly
# alcohol_direct <- read_csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/alcohol-consumption/drinks.csv")

```


What are the variable types? Any missing values we should worry about? 

The dataset contains five variables, which are devided among three types (logical, numerical and character). There are no missing values.

```{r glimpse_skim_data}
glimpse(drinks)
skim(drinks)

```

Make a plot that shows the top 25 beer consuming countries

```{r beer_plot}
drinks %>%
  slice_max(order_by = beer_servings, n=25) %>%
  mutate(country = fct_reorder(country, beer_servings)) %>%
  ggplot(aes(x=beer_servings, y=country))+
  geom_col(fill='#EBC934')+
  labs(title = "You like beer? These countries do too!",
         subtitle = "Top 25 beer consuming countries",
         x = "Servings per person", 
         y = "",
         caption = "Source: https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/")+
  NULL
```


Make a plot that shows the top 25 wine consuming countries

```{r wine_plot}

drinks %>%
  slice_max(order_by = wine_servings, n=25) %>%
  mutate(country = fct_reorder(country, wine_servings)) %>%
  ggplot(aes(x=wine_servings, y=country))+
  geom_col(fill='#9C0606')+
  labs(title = "Of course, the French like their wine. \
But what other countries consume most of it?",
         subtitle = "Top 25 wine consuming countries",
         x = "Servings per person", 
         y = "",
         caption = "Source: https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/")+
  NULL
```


Finally, make a plot that shows the top 25 spirit consuming countries
```{r spirit_plot}
drinks %>%
  slice_max(order_by = spirit_servings, n=25) %>%
  mutate(country = fct_reorder(country, spirit_servings)) %>%
  ggplot(aes(x=spirit_servings, y=country))+
  geom_col(fill='#42069C')+
  labs(title = "The number one spirit consumer country will surprise you!\
The others, maybe not",
         subtitle = "Top 25 spirits consuming countries",
         x = "Servings per person", 
         y = "",
         caption = "Source: https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/")+
  NULL
```
What can you infer from these plots? Don't just explain what's in the graph, but speculate or tell a short story (1-2 paragraphs max).


> 
The graphs represent the consumption per person of the three different bevarages in the top 25 consuming countries. 
They also highlight some strong stereotipes we might have about different countries. For instance, you would be lying if you said that you were not surprised by not seeing Germany as the top 1 country consuming beer! We were surprised as well! In fact, after researching Namibia's history for a while, we discovered that Namibia was a German colony and went through a series of wars to finally get independent in the late 1990. Drinking beer and alchool was prohibited during this time, so after becoming independent Namibia's population has adopted beer as a symbol of aparheid and independence.
Having people from France, Italy, and Spain in our study group, we were expecting to see those countries among the top 25 consuming countries of wine. So, no surprise on this end. 
Regarding spirit consumption, we all knew how much Russians love spirits, what we did not know was that the average daily intake of alcohol in Grenada is 40.4 grams of pure alcohol: 7 grams higher than the  Average intake worldwide. 


# Analysis of movies- IMDB dataset

We will look at a subset sample of movies, taken from the [Kaggle IMDB 5000 movie dataset](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset)

```{r,load_movies, warning=FALSE, message=FALSE}

movies <- read_csv(here::here("data", "movies.csv"))
glimpse(movies)

```

Besides the obvious variables of `title`, `genre`, `director`, `year`, and `duration`, the rest of the variables are as follows:

- `gross` : The gross earnings in the US box office, not adjusted for inflation
- `budget`: The movie's budget 
- `cast_facebook_likes`: the number of facebook likes cast memebrs received
- `votes`: the number of people who voted for (or rated) the movie in IMDB 
- `reviews`: the number of reviews for that movie
- `rating`: IMDB average rating 

- Are there any missing values (NAs)? Are all entries distinct or are there duplicate entries?

There are no missing values in the dataset. There are, however, 54 duplicates (number of rows - number of unique titles).

- Produce a table with the count of movies by genre, ranked in descending order
```{r}
table0 <- movies %>% 
  group_by(genre)%>%
  count(sort=TRUE)
table0
```

- Produce a table with the average gross earning and budget (`gross` and `budget`) by genre. Calculate a variable `return_on_budget` which shows how many $ did a movie make at the box office for each $ of its budget. Ranked genres by this `return_on_budget` in descending order

```{r}
movies %>% 
  group_by(genre) %>% 
  summarise(mean_gross = mean(gross), mean_budget = mean(budget),
            return_on_budget =mean_gross/mean_budget)  %>% 
  arrange(desc(return_on_budget)) 
  NULL

```

- Produce a table that shows the top 15 directors who have created the highest gross revenue in the box office. Don't just show the total gross amount, but also the mean, median, and standard deviation per director.

```{r}
movies %>%
   group_by(director)%>%
  summarise(director_gross=sum(gross), mean(gross), median(gross), max(gross)) %>%
  slice_max(order_by= director_gross, n=15)

```

- Finally, ratings. Produce a table that describes how ratings are distributed by genre. We don't want just the mean, but also, min, max, median, SD and some kind of a histogram or density graph that visually shows how ratings are distributed. 
- 
```{r}
rating_genre <- movies %>%
  group_by(genre) %>%
  summarise(mean = mean(rating), min = min(rating), max = max(rating), median = median(rating), 
            stdev = StdDev(rating), count= count(genre))%>%
  arrange(desc(mean))
rating_genre

rating_genre %>%
  ggplot(aes( x= mean, y= fct_reorder(genre, mean))) +
  geom_col(fill ="#42069C") +
  labs(title = "How are the ratings of different genres of movies distributed? These types of movies always get high scores!",
         subtitle = "Rating distribution of different genres of movies",
         x = "Average rating", 
         y = "",
         caption = "https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset")+
  NULL
```


  - Examine the relationship between `gross` and `cast_facebook_likes`. Produce a scatterplot and write one sentence discussing whether the number of facebook likes that the cast has received is likely to be a good predictor of how much money a movie will make at the box office. What variable are you going to map to the Y- and X- axes?
      
```{r, gross_on_fblikes}
skim(movies)
table <- select(movies,c("gross","cast_facebook_likes"))

ggplot(movies, aes(x=gross , y=cast_facebook_likes))+
  geom_point()+
  xlim(0,400000000) +
  ylim(0,200000)+ 
  labs(title = "Is the gross earnings of the movie related to facebook likes?",
         subtitle = "Relationship between gross earnings and facebook likes",
         x = "Gross earnings of movies", 
         y = "The number of facebook likes cast members received",
         caption = "https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset")+
  geom_smooth(method = "lm")
```

The graph does not depict a clear and strong correlation between the number of facebook likes that the cast has received and the gross earnings of the movie. Therefore, we can conclude that it is not a good predictor of how much money a movie will make at the box office.
Furthermore, we plotted the facebook likes with the gross earnings adding a linear regression: as we can see in the graph below, the fitted model is not accurate representation of the data. Sequantally, we cannot conclude anything about the relationship between the two variables.

```{r}
ggplot(movies, aes(x=gross, y=cast_facebook_likes)) +
  geom_point()+
  xlim(0, 300000000)+
  ylim(0, 100000)+
  labs(title = "Is the gross earnings of the movie related to facebook likes?",
         subtitle = "Relationship between gross earnings and facebook likes",
         x = "Gross earnings of movies", 
         y = "The number of facebook likes cast members received",
         caption = "https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset")+
  geom_smooth(method='lm')

summary(lm(cast_facebook_likes ~ gross, movies))
```

- Examine the relationship between `gross` and `budget`. Produce a scatterplot and write one sentence discussing whether budget is likely to be a good predictor of how much money a movie will make at the box office.

```{r, gross_on_budget}

ggplot(movies, aes(x=gross , y=budget))+
  geom_point()+
  labs(title = "Is the gross earnings of the movie related to movie budgets?",
         subtitle = "Relationship between gross earnings and movie budgets",
         x = "Gross earnings of movies", 
         y = "The movie's budget",
         caption = "https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset")+
  geom_smooth(method= "lm")
```

While the scales of the two variables are extremely different, we can say that the higher the budget, the higher the gross earnings. 

  
  - Examine the relationship between `gross` and `rating`. Produce a scatterplot, faceted by `genre` and discuss whether IMDB ratings are likely to be a good predictor of how much money a movie will make at the box office. Is there anything strange in this dataset?

```{r, gross_on_rating}
ggplot(movies, aes(x=gross , y=rating))+
  geom_point()+
  labs(title = "Is the gross earnings of the movie related to movie ratings?",
         subtitle = "Relationship between gross earnings of movies and movie ratings",
         x = "Gross earnings of movies", 
         y = "Movie ratings",
         caption = "https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset")+
  geom_smooth(method ="lm")+
  facet_wrap(~genre)
```

The dataset presents some limitations: as we can see from "table0", there are many genres that are underrepresented in the number of datapoints (Thriller 1, Western 2, Romance 2, Musical 2, Family 3 vs Comedy 848 e.g.). This does not allow to make the same studies/comparisons between the different genres.
To answer the question, we cannot infer a sure relationship between the IMDB ratings and the gross earnings for every genre.
For the ones with enough datapoints, we observe a positive correlation between the ratings and the gross earnings, especially for Action, Horror, Drama, Adventure. 

# Returns of financial stocks

In this task we will try to understand how chosen tickers compare in risk and return.
We chose the following list of tickers from the dataset "nyse.csv" and then compared their performance with the one of SPY:
"KO": Coca-Cola Company (The)
"ACN": Accenture plc
"BA": Boeing Company (The)
"GS": Goldman Sachs Group, Inc. (The)
"NKE": Nike, Inc.
"NVO": Novo Nordisk A/S

```{r load_nyse_data, message=FALSE, warning=FALSE}
nyse <- read_csv(here::here("data","nyse.csv"))
skim(nyse)
```

Based on this dataset, create a table and a bar plot that shows the number of companies per sector, in descending order

```{r companies_per_sector}
companies_per_sector <- nyse  %>%
  group_by(sector)  %>%
  summarise(count=count(sector))  %>%
  arrange(desc(count))

companies_per_sector %>%
  ggplot(aes( x= count, y= fct_reorder(sector, count))) +
  geom_col(fill= "#42069C") +
  labs(title = "How are the numbers of different sectors of company \
distributed? ",
         subtitle = "Number of companies per sector ",
         x = "Number of companies", 
         y = "",
         caption = "https://mam2022.netlify.app/reference/finance_data/")+
  geom_smooth(method ="lm")+
 NULL
```

```{r get_price_data, message=FALSE, warning=FALSE, cache=TRUE}
myStocks <- c("KO","ACN","BA","GS","NKE","NVO","SPY" ) %>%
  tq_get(get  = "stock.prices",
         from = "2011-01-01",
         to   = "2021-08-31") %>%
  group_by(symbol) 
  NULL

glimpse(myStocks) # examine the structure of the resulting data frame
skim(myStocks)
```

Financial performance analysis depends on returns; If I buy a stock today for 100 and I sell it tomorrow for 101.75, my one-day return, assuming no transaction costs, is 1.75%. So given the adjusted closing prices, our first step is to calculate daily and monthly returns.

```{r calculate_returns, message=FALSE, warning=FALSE, cache=TRUE}
#calculate daily returns
myStocks_returns_daily <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "daily", 
               type       = "log",
               col_rename = "daily_returns",
               cols = c(nested.col))  

#calculate monthly  returns
myStocks_returns_monthly <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               type       = "arithmetic",
               col_rename = "monthly_returns",
               cols = c(nested.col)) 

#calculate yearly returns
myStocks_returns_annual <- myStocks %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "yearly", 
               type       = "arithmetic",
               col_rename = "yearly_returns",
               cols = c(nested.col))
```

Create a table where you summarise monthly returns for each of the stocks and `SPY`; min, max, median, mean, SD.

```{r summarise_monthly_returns}
stock_returns_monthly <- myStocks_returns_monthly %>%
  group_by(symbol) %>%
  summarise(min=min(monthly_returns), mix=max(monthly_returns), mean=mean(monthly_returns), stdv= StdDev(monthly_returns))

stock_returns_monthly
  
```

Plot a density plot, using `geom_density()`, for each of the stocks
```{r density_monthly_returns}

myStocks_returns_monthly  %>%
  ggplot(aes(x=monthly_returns))  +
  geom_density(color="#42069C")+
  labs(title = "Monthly returns of each of stock ",
         x = "Monthly returns", 
         y = "",
         caption = "https://mam2022.netlify.app/reference/finance_data/")+
  facet_wrap(~symbol)

```

What can you infer from this plot? Which stock is the riskiest? The least risky? 
>  
The sharpness of the curve gives us information of the distribution around the mean of the returns. This inharently represents variance, which is also an indicator of risk. 
We can conclude that:
1. The SP500 Index is the least risky since it is also the sharpest bell. 
2. The two stocks with the highest volatility and therefore the most risky stocks are Boeing (BA) and Goldman Sachs (GS). We could explain the high volatility in returns of BA by looking at the uncertainty in the air travel industry mostly cause by the global pandemic but also other historical events.

Finally, make a plot that shows the expected monthly return (mean) of a stock on the Y axis and the risk (standard deviation) in the X-axis. Please use `ggrepel::geom_text_repel()` to label each stock

```{r risk_return_plot}
stock_returns_monthly %>%
  ggplot(aes(x=stdv, y= mean))+
  geom_point(color="#42069C") +
  labs(title = "High return means high risk? See what are outliers!",
        subtitle = "Relationship between expected monthly return and risk",
         x = "Monthly returns", 
         y = "Risk",
         caption = "https://mam2022.netlify.app/reference/finance_data/")+
  ggrepel::geom_text_repel(aes(label = symbol),
                     nudge_x = 0,
                     na.rm = TRUE)
```

What can you infer from this plot? Are there any stocks which, while being riskier, do not have a higher expected return?

> 
We would expect to see an ideal investment with high returns and a relatively low risk at the top left corner of this graph. 
The different points that we obtained allow us to visually compare the selected stocks in terms of risk and return. Looking at the graph, it becomes clear that some stocks are riskier but do not have higher expected returns compared to the peers; This is the case of GS and BA. We find GS to be a particular poor investment compared to the others since it has the second highest risk while having only the second lowest expected return.


# On your own: IBM HR Analytics

## IBM HR Dataset
### Description of key statistics

This data set is a fictional dataset created by IBM data scientists. It provides data on key human resources variables, such as income, age, education, gender, attrition and others. The dataset contains 1470 observations for 19 variables, and it has no missing values. We will deep dive into some key statistics to better understand the fictional IBM Workplace. 

Firstly we will look into attrition, which refers to the percentage of people within the dataset that have left the country. Out of the whole dataset, 237 employees left the company, which results in an attrition of 16.1%.The age distribution for people who left the company can be seen in Figure 1. If we compare this distribution to the close to normal age distribution of the whole company in Figure 2, we see that they clearly differ. Younger and older employees are most likely to leave, as the percentage of leavers for those age groups is higher than their percentage of total employees. 


```{r, fig.show="hold", out.width="30%"}

hr_dataset <- read_csv(here::here("data", "datasets_1067_1925_WA_Fn-UseC_-HR-Employee-Attrition.csv"))
#glimpse(hr_dataset)
```



```{r, fig.show="hold", out.width="30%"}

hr_cleaned <- hr_dataset %>% 
  clean_names() %>% 
  mutate(
    education = case_when(
      education == 1 ~ "Below College",
      education == 2 ~ "College",
      education == 3 ~ "Bachelor",
      education == 4 ~ "Master",
      education == 5 ~ "Doctor"
    ),
    environment_satisfaction = case_when(
      environment_satisfaction == 1 ~ "Low",
      environment_satisfaction == 2 ~ "Medium",
      environment_satisfaction == 3 ~ "High",
      environment_satisfaction == 4 ~ "Very High"
    ),
    job_satisfaction = case_when(
      job_satisfaction == 1 ~ "Low",
      job_satisfaction == 2 ~ "Medium",
      job_satisfaction == 3 ~ "High",
      job_satisfaction == 4 ~ "Very High"
    ),
    performance_rating = case_when(
      performance_rating == 1 ~ "Low",
      performance_rating == 2 ~ "Good",
      performance_rating == 3 ~ "Excellent",
      performance_rating == 4 ~ "Outstanding"
    ),
    work_life_balance = case_when(
      work_life_balance == 1 ~ "Bad",
      work_life_balance == 2 ~ "Good",
      work_life_balance == 3 ~ "Better",
      work_life_balance == 4 ~ "Best"
    )
  ) %>% 
  select(age, attrition, daily_rate, department,
         distance_from_home, education,
         gender, job_role,environment_satisfaction,
         job_satisfaction, marital_status,
         monthly_income, num_companies_worked, percent_salary_hike,
         performance_rating, total_working_years,
         work_life_balance, years_at_company,
         years_since_last_promotion)

```

```{r, fig.show="hold", out.width="30%"}

number_of_attrition <- hr_cleaned  %>% 
  filter(attrition == "Yes")

number_of_attrition%>%
  ggplot(aes(x=age, y=count(age)))+
  geom_col(fill="#42069C")

number_of_attrition=number_of_attrition%>%
  count()

total_number <- hr_cleaned %>% 
  count()

#the relative rate of attrition would be 
attrition = number_of_attrition/ total_number

#age
hr_cleaned%>%
  ggplot(aes(x=age)) +
  geom_bar(fill="#42069C")+
  labs(title = "The distribution of employee age",
       x = "Age", 
       y = "Count",
       caption = "https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset")

```

Some other key variable distributions can be observed in the figures below, all right-skewed. Years since last promotion shows almost exponential distribution tendency, with most employees receiving a promotion within the last year. There seems to be a big drop in employees with more than 10 years working at the company, and most employees make between USD 9k and USD 1k per month.

```{r, fig.show="hold", out.width="30%"}

#years_at_company
hr_cleaned%>%
  ggplot(aes(x=years_at_company))+
  geom_bar(fill="#42069C") +
  labs(title = "The distribution of employee's tenure",
       x = "Employee tenure", 
       y = "Count",
       caption = "https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset")

#monthly_income is continuous variable so we do density
hr_cleaned%>%
  ggplot(aes(x=monthly_income))+
  geom_density(fill="#42069C") +
  labs(title = "The density of employee's monthly income",
       x = "Monthly income of employees", 
       y = "",
       caption = "https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset")


#years_since_last_promotion
hr_cleaned%>%
  ggplot(aes(x=years_since_last_promotion))+
  geom_bar(fill="#42069C") +
  labs(title = "The distribution of years since the last promotion",
       x = "Years since last promotion", 
       y = "Count",
       caption = "https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset")
```

With regards to employees’ wellness, over 60% of them reported high or very high job satisfaction, while most people report a better state of work-life balance. When we look at the performance between groups of employees that report different satisfaction and balance, there does not seem to be a difference in the proportion of employees that score outstanding, or excellent(CHECK PERCENTAGES).

```{r, fig.show="hold", out.width="30%"}
hr_cleaned%>%
  group_by(job_satisfaction)%>%
  summarise(percentage = count(job_satisfaction))%>%
  mutate(percentage = percentage/sum(percentage)*100)%>%
  ggplot(aes(x=job_satisfaction, y = percentage))+
  geom_col(fill="#42069C")+
  labs(title = "The distribution of different job satisfaction level",
       x = "Job satisfaction", 
       y = "Percentage",
       caption = "https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset")
  

hr_cleaned%>%
  group_by(performance_rating)%>%
  ggplot(aes(x=performance_rating, y=job_satisfaction))+
  geom_col(fill="#42069C")+
  facet_wrap(~job_satisfaction)+
  labs(title = "The relationship between job satisfaction and perfomance",
       x = "Job satisfaction", 
       y = "",
       caption = "https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset")

hr_cleaned%>%
  group_by(work_life_balance)%>%
  summarise(percentage = count(work_life_balance))%>%
  mutate(percentage = percentage/sum(percentage)*100)%>%
  ggplot(aes(x=work_life_balance, y = percentage))+
  geom_col(fill="#42069C")+
  labs(title = "The distribution of different work-life balance level",
       x = "Work-life balance", 
       y = "Percentage",
       caption = "https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset")
  

hr_cleaned%>%
  group_by(performance_rating)%>%
  ggplot(aes(x=performance_rating, y=work_life_balance))+
  geom_col(fill="#42069C")+
  facet_wrap(~work_life_balance)+
  labs(title = "The relationship between work-life balance and\
performance",
       x = "Performance", 
       y = "",
       caption = "https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset")

```

From the plot of gender vs monthly income we see that for women, monthly income is around USD 6500 while for men it stands at USD 6250. So it’s clear that there is some income difference between females and males but it is almost negligible (USD 250). Monthly income varies directly with the education of employees. So employees with a doctorate are generally paid the highest monthly income.

```{r, fig.show="hold", out.width="30%"}

hr_cleaned%>%
  group_by(education)%>%
  summarise(monthly_income=mean(monthly_income))%>%
  ggplot(aes(x=monthly_income, y=fct_reorder(education, monthly_income)))+
  geom_col(fill = "#42069C")  +
  labs(title = "The relationship between monthly income and education",
       x = "Monthly income", 
       y = "",
       caption = "https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset")

hr_cleaned%>%
  group_by(gender)%>%
  summarise(monthly_income=mean(monthly_income))%>%
  ggplot(aes(x=monthly_income, y=fct_reorder(gender, monthly_income)))+
  geom_col(fill = "#42069C")+
  labs(title = "The relationship between monthly income and gender",
       x = "Monthly income", 
       y = "",
       caption = "https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset")

```

Employees with managerial roles are generally paid higher income, compared to the others, owing to the importance of their roles and the amount of responsibility they hold.

```{r, fig.show="hold", out.width="30%"}
hr_cleaned %>%
  arrange(desc(monthly_income))%>%
  ggplot(aes(x=monthly_income, y=fct_reorder(job_role, monthly_income)))+
  geom_boxplot(color = "#42069C") +
  labs(title = "The relationship between monthly income and job\
roles",
       x = "Monthly income", 
       y = "",
       caption = "https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset")

```
Monthly income generally increases with age, except for the sales representative and laboratory technicians. This could be because, with more experience, the employees can move to the roles with more responsibility which leads to higher monthly income. For sales representatives and Lab technicians, income increase is almost negligible with age. This could be because, for these roles, more work experience doesn’t necessarily guarantee more skills learnt for them to move to better-paying roles.

```{r, fig.show="hold", out.width="30%"}
hr_cleaned %>%
  #arrange(desc(monthly_income))%>%
  ggplot(aes( x= age, y= monthly_income))+ facet_wrap(vars(job_role))+
  geom_point(color="#42069C")+ geom_smooth(method="lm")+
  labs(title = "Monthly income by age for different positions",
       x = "Age", 
       y = "Monthly income",
       caption = "https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset")
```


# Challenge 1: Replicating a chart

The purpose of this exercise is to reproduce a plot using your `dplyr` and `ggplot2` skills. Read the  article [The Racial Factor: There's 77 Counties Which Are Deep Blue But Also Low-Vaxx. Guess What They Have In Common?](https://acasignups.net/21/07/18/racial-factor-theres-77-counties-which-are-deep-blue-also-low-vaxx-guess-what-they-have) and have a look at the attached figure.

```{r challenge1, out.width="90%"}
#knitr::include_graphics(here::here("images", "vaxxes_by_state_red_blue_every_county_070321_1.jpeg"), error = FALSE)

```



1. To get vaccination by county, we will use [data from the CDC](https://data.cdc.gov/Vaccinations/COVID-19-Vaccinations-in-the-United-States-County/8xkx-amqh)
2. You need to get [County Presidential Election Returns 2000-2020](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/VOQCHQ)
3. Finally, you also need an estimate of the [population of each county](https://www.ers.usda.gov/webdocs/DataFiles/48747/PopulationEstimates.csv?v=2232)
 

```{r, cache=TRUE}

# Download CDC vaccination by county
cdc_url <- "https://data.cdc.gov/api/views/8xkx-amqh/rows.csv?accessType=DOWNLOAD"
vaccinations <- vroom(cdc_url) %>% 
  janitor::clean_names() %>% 
  filter(fips != "UNK") # remove counties that have an unknown (UNK) FIPS code

# Download County Presidential Election Returns
# https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/VOQCHQ
election2020_results <- vroom(here::here("data", "countypres_2000-2020.csv")) %>% 
  janitor::clean_names() %>% 
  
  # just keep the results for the 2020 election
  filter(year == "2020") %>% 
  
  # change original name county_fips to fips, to be consistent with the other two files
  rename (fips = county_fips)

# Download county population data
population_url <- "https://www.ers.usda.gov/webdocs/DataFiles/48747/PopulationEstimates.csv?v=2232"
population <- vroom(population_url) %>% 
  janitor::clean_names() %>% 
  
  # select the latest data, namely 2019
  select(fips = fip_stxt, pop_estimate_2019) %>% 
  
  # pad FIPS codes with leading zeros, so they are always made up of 5 characters
  mutate(fips = stringi::stri_pad_left(fips, width=5, pad = "0"))

```




```{r}
#we are going to take a look at the data first and then we will have to merge the data

population_vaccinations <- left_join(population, vaccinations, by="fips")


pve <- left_join(population_vaccinations, election2020_results, by="fips")
#we will group pve by county
```

```{r}

pve %>%
  filter(date =="08/03/2021") %>%
  filter(candidate == "DONALD J TRUMP") %>%
  filter(series_complete_pop_pct > 1) %>%
  mutate(percentage_trump= candidatevotes/totalvotes*100) %>%
  filter(percentage_trump > 1) %>%
  ggplot(aes(x=percentage_trump, y=series_complete_pop_pct)) +
  geom_point(aes(size = pop_estimate_2019))+
  scale_size_continuous(range = c(0.01, 5))+
  xlim(0,100)+
  ylim(0,100)+
  labs(title = "COVID-19 VACCINATION LEVELS OUT OF TOTAL POPULATION BY COUNTY",
        subtitle = "Most states on FULLY vaccinated only;CA, GA, IA, MI & TX based on total doses administered",
         x = "2020 Trump Vote %", 
         y = "Percentage of Total Population Vaccinated",
         caption = "Centers for Disease Control, COVID Act NOW, state health depts")

pve

```


# Challenge 2: Opinion polls for the 2021 German elections

The Guardian newspaper has an [election poll tracker for the upcoming German election](https://www.theguardian.com/world/2021/aug/20/german-election-poll-tracker-who-will-be-the-next-chancellor).
The list of the opinion polls since Jan 2021 can be found at [Wikipedia](https://en.wikipedia.org/wiki/Opinion_polling_for_the_2021_German_federal_election) and your task is to reproduce the graph similar to the one produced by the Guardian. 


The following code will scrape the wikipedia page and import the table in a dataframe.


```{r, scrape_wikipedia_polling_data, warnings= FALSE, message=FALSE}
url <- "https://en.wikipedia.org/wiki/Opinion_polling_for_the_2021_German_federal_election"
# https://www.economist.com/graphic-detail/who-will-succeed-angela-merkel
# https://www.theguardian.com/world/2021/jun/21/german-election-poll-tracker-who-will-be-the-next-chancellor


# get tables that exist on wikipedia page 
tables <- url %>% 
  read_html() %>% 
  html_nodes(css="table")


# parse HTML tables into a dataframe called polls 
# Use purr::map() to create a list of all tables in URL
polls <- map(tables, . %>% 
             html_table(fill=TRUE)%>% 
             janitor::clean_names())


# list of opinion polls
german_election_polls <- polls[[1]] %>% # the first table on the page contains the list of all opinions polls
  slice(2:(n()-1)) %>%  # drop the first row, as it contains again the variable names and last row that contains 2017 results
  mutate(
         # polls are shown to run from-to, e.g. 9-13 Aug 2021. We keep the last date, 13 Aug here, as the poll date
         # and we extract it by picking the last 11 characters from that field
         end_date = str_sub(fieldwork_date, -11),
         
         # end_date is still a string, so we convert it into a date object using lubridate::dmy()
         end_date = dmy(end_date),
         
         # we also get the month and week number from the date, if we want to do analysis by month- week, etc.
         month = month(end_date),
         week = isoweek(end_date)
         )
```



```{r}
german_election_polls %>%
  select(end_date,union, grune, spd, af_d, linke, fdp)%>%
  ggplot(aes(x=end_date, y= union))+
  geom_point()+
  geom_point(data=german_election_polls, aes(x=end_date, y= grune, color= "Green")) +
  geom_point(data=german_election_polls, aes(x=end_date, y= spd, color= "Red")) +
  geom_point(data=german_election_polls, aes(x=end_date, y= af_d, color= "Blue")) +
  geom_point(data=german_election_polls, aes(x=end_date, y= linke, color= "Purple")) +
  geom_point(data=german_election_polls, aes(x=end_date, y= fdp, color= "Yellow"))
 
german_election_polls %>%
  select(polling_firm, end_date,union, grune, spd, af_d, linke, fdp) %>%
  ggplot(aes(x=end_date, y=grune))+
  geom_point(color = "green")+
  geom_point(data = german_election_polls, aes(x=end_date, y=spd,color = "red"))+
  geom_point(data = german_election_polls, aes(x=end_date, y=af_d,color = "blue"))+
  geom_point(data = german_election_polls, aes(x=end_date, y=linke,color = "purple"))+
  geom_point(data = german_election_polls, aes(x=end_date, y=fdp,color = "yellow"))+
  geom_point(data = german_election_polls, aes(x=end_date, y=union))+
  geom_smooth(data = german_election_polls, aes(x=end_date, y=grune,color = "green", method="lm"))+
  geom_smooth(data = german_election_polls, aes(x=end_date, y=spd,color = "red", method="lm"))+
  geom_smooth(data = german_election_polls, aes(x=end_date, y=af_d,color = "blue", method="lm"))+
  geom_smooth(data = german_election_polls, aes(x=end_date, y=linke,color = "purple", method="lm"))+
  geom_smooth(data = german_election_polls, aes(x=end_date, y=fdp,color = "yellow", method="lm"))+
  geom_smooth(data = german_election_polls, aes(x=end_date, y=union, method="lm"))

```

```{r}
library(zoo)

rolling_dataset<- german_election_polls %>%
      select(polling_firm, end_date,week,union, grune, spd, af_d, linke, fdp) %>%
      arrange(desc(-week))%>%
      dplyr::mutate(rolling_spd = zoo::rollmean(spd, k = 14, fill = NA)) %>%
      dplyr::mutate(rolling_union = zoo::rollmean(union, k = 14, fill = NA)) %>%
      dplyr::mutate(rolling_grune = zoo::rollmean(grune, k = 14, fill = NA)) %>%
      dplyr::mutate(rolling_af_d = zoo::rollmean(af_d, k = 14, fill = NA)) %>%
      dplyr::mutate(rolling_linke = zoo::rollmean(linke, k = 14, fill = NA)) %>%
      dplyr::mutate(rolling_fdp = zoo::rollmean(fdp, k = 14, fill = NA))  
  
rolling_dataset %>%
  ggplot(aes(x=end_date, y=rolling_spd))+
      geom_line(color = "red")+
      geom_point(data = german_election_polls, aes(x=end_date, y=spd,color = "red"))+
      geom_line(data = rolling_dataset, aes(x=end_date, y=rolling_union))+
      geom_point(data = german_election_polls, aes(x=end_date, y=union))+
      geom_line(data = rolling_dataset, aes(x=end_date, y=rolling_grune, color = "green"))+
      geom_point(data = german_election_polls, aes(x=end_date, y=grune, color= "green"))+
      geom_line(data = rolling_dataset, aes(x=end_date, y=rolling_af_d, color = "blue"))+
      geom_point(data = german_election_polls, aes(x=end_date, y=af_d, color ="blue"))+
      geom_line(data = rolling_dataset, aes(x=end_date, y=rolling_linke, color = "purple"))+
      geom_point(data = german_election_polls, aes(x=end_date, y=linke, color ="purple"))+
      geom_line(data = rolling_dataset, aes(x=end_date, y=rolling_fdp, color = "yellow"))+
      geom_point(data = german_election_polls, aes(x=end_date, y=fdp, color ="yellow"))+
      labs(title = "German election poll tracker: who will be the next chancellor?",
        subtitle = "Find out who is leading the polling to succeed Angela Merkel as chancellor of Germany",
         x = "Date", 
         y = "Rolling average percentage",
         caption = "Source: wahirecht.de")
```


# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed R Markdown file as an HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas.

# Details

- Who did you collaborate with: We collaborated with each other, no external collaborations. 
- Approximately how much time did you spend on this problem set: Thursday 1 h, Friday 7 h, Saturday 5h - 13 hours
- What, if anything, gave you the most trouble: Challenge nr. 2: finding a way to incorporate the 14-days average of the parties while plotting the daily polling results per party was challenging.  


**Please seek out help when you need it,** and remember the [15-minute rule](https://mam2022.netlify.app/syllabus/#the-15-minute-rule){target=_blank}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!  

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else? 


# Rubric

Check minus (1/5): Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. 

Check (3/5): Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). 

Check plus (5/5): Finished all components of the assignment correctly and addressed both challenges. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output.









